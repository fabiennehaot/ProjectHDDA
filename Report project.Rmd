---
title: "HDDA Project"
author: "Lisa Barbier, Klara Dewitte, Fabienne Haot, Kasimir Putseys"
date: "December 2021"
bibliography: references.bib
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(gridExtra)
library(HDDAData)
library(glmnet)
library(MASS)
library(locfdr)
library(boot)
library(pROC)
```

# Research Question

# Executive Summary

# Technical Report

## Data

Data description in [@einecke:2010]
```{r getdata}
data("Einecke2010Kidney")
X_raw <- Einecke2010Kidney[, -1]
Y <- factor(Einecke2010Kidney[, 1], 
            levels = c(0,1),labels = c('accept','reject'))
```

## Exploration of the Data

## Hypothesis Testing 
Not every gene in the data set is an indicator for kidney rejection. In this section we find out which genes behave differently between the two kidney rejection groups. Since we do not know a priori whether differentiated genes will show higher or lower intensities we opt for 2 sided t-tests with unequal variances. 

\[H_0: \mu_{normal}(gene) = \mu_{reject}(gene) \]
\[H_1: \mu_{normal}(gene) \neq \mu_{reject}(gene) \]

We first perform these t-tests for every gene in the data, with an individual significance level of 5%. Because we are performing multiple testing we would expect to find about 500 genes with a p-value lower than 5% if the null hypothesis holds for all the genes. 

```{r hyptest_unadjusted, fig.height= 4}
gene_data <- as.matrix(X_raw)
group <- Y
# non adjusted p-values 
ttest_results <- apply(gene_data, 2, function(x) {
  t_test <- t.test(x ~ group)
  p_val <- t_test$p.value
  stat <- t_test$statistic
  df <- t_test$parameter
  z_val <- case_when(stat < 0 ~ qnorm(p_val/2), TRUE~ qnorm(1-p_val/2))
  tibble(stat = stat, p_val = p_val,z_val = z_val, df = df)})
t_stats <- bind_rows(ttest_results) %>%
  mutate(gene = colnames(X_raw))

# selection
nonadj_level <- 0.05
selected_nonadj <- t_stats %>% filter(p_val< nonadj_level)

# plot histogram for the unadjusted z-values
mean_z <- round(mean(t_stats$z_val),3) 
sd_z <- round(sd(t_stats$z_val),3) 
threshold_nonadj <- selected_nonadj %>% 
  summarize(left = max(case_when(z_val > mean_z ~ -Inf,TRUE~ z_val)),
            right = min(case_when(z_val < mean_z ~ +Inf,TRUE~ z_val)))

hist_zval <- ggplot(data=t_stats) + 
  geom_histogram(
    mapping = aes(x = z_val,after_stat(density)),
    bins = 20, alpha = .7, fill='lightblue') +
  geom_vline(mapping=aes(xintercept = threshold_nonadj$left),color= 'red')+    
  geom_vline(mapping=aes(xintercept = threshold_nonadj$right),color= 'red')+
  geom_line(mapping=aes(x = z_val,y = dnorm(z_val)),color='blue')+
  labs(title = "multiple t-tests unadjusted",
       subtitle="histogram of corresponding z-values",
       x = "z-values",
       y= "probability density")+
  annotate(geom="text", x=-7, y=0.38, 
           label=paste0("mean: ", mean_z,"\nstdev:", sd_z))+
  theme_bw()

# signal histogram - current behaviour of p-values: uniform?
hist_pval <- ggplot(data = t_stats) +
  geom_histogram(mapping = aes(x = p_val),
                 fill = "firebrick", breaks = seq(0,1,.05),alpha=.7) +
  labs(title = 'histogram of p-values') +
  theme_bw()

grid.arrange(hist_zval, hist_pval, ncol=2)

# number of differentiated genes
table_p_nonadj <- table(t_stats$p_val < nonadj_level)
nr_detected_nonadj <- as.numeric(table_p_nonadj['TRUE'])

```

There are `r nr_detected_nonadj` out of 10000 genes with non-adjusted p-values below 0.05. The selected genes are mostly found in the lower tail, the negative z-values indicate higher intensities in the group with rejected kidneys.

The distribution is not standard normal as the observations are skewed to the left, the standard deviation is also much larger than 1.

The histogram of the p-values is not uniform, a clear peak is visible for p-values lower than 5%, but the frequencies for values between 5% and 35% drop too slowly, again because of the large spread of the distribution.

We will standardize the z values, recalculate the associated p-values and adjust for multiple testing by controlling the FDR to 10%. The p-values are adjusted using the @BH:1995 procedure. 

```{r hyptest_fdr_padj}
# adjusted p values
t_stats <- t_stats %>%
  mutate(z_scale = (z_val - mean(z_val))/sd(z_val),
         p_scale = pnorm(z_scale),
         p_adj = p.adjust(p_scale, method="fdr"))

# these are the detected genes and a histogram of their t-values
fdr_level <- 0.1
selected_fdr <- t_stats %>% filter(p_adj < fdr_level)

# plot histogram
threshold_fdr <- selected_fdr %>% 
  summarize(left = max(case_when(z_scale > 0~-Inf,TRUE~ z_scale)),
            right = min(case_when(z_scale < 0~+Inf,TRUE~ z_scale)))

hist_fdr <- ggplot(t_stats) + 
  geom_histogram(mapping=aes(x = z_scale, after_stat(density)),
                             fill="lightblue", bins = 20, alpha=.7)+
  geom_vline(mapping=aes(xintercept=threshold_fdr$left),color='red') +    
  geom_line(mapping = aes(x=z_scale, y= dnorm(z_scale)),color='blue') +
  labs(title = "multiple t-tests FDR",
       caption = 'histogram of transformed z-values, rescaled and adjusted for multiple testing')+
  theme_bw()            

# signal histogram - current behaviour of p-values: uniform?
hist_pscale <- ggplot(data = t_stats) +
  geom_histogram(mapping = aes(x = p_scale),
                 fill = "firebrick", breaks = seq(0,1,.05),alpha=.7) +
  labs(title = 'histogram of p-values') +
  theme_bw()

# monotonous transformation:
pp_adj <- t_stats %>%
  ggplot(aes(x = p_scale,y = p_adj)) +
  geom_point(size = .3,color='blue') +
  geom_segment(x=0,y=0,xend=1,yend=1) +
  geom_hline(yintercept = fdr_level, color='red') +
  labs(title = "Adjusting the p-values",
       y= "adjusted p-value (BH, 1995)") +
  theme_bw()

# show histogram z-values +  monotonous adjustment for the p-values
grid.arrange(hist_fdr, hist_pscale, pp_adj, ncol=3)

# number of detected genes
table_p_adj <- table(t_stats$p_adj < fdr_level)
nr_detected_fdr <- as.numeric(table_p_adj['TRUE'])

```

The scaling has lowered the number of candidate genes drastically.
There are now `r nr_detected_fdr` genes with adjusted p-value below 10%. All of the detected genes are now found in the lower tail. 

The histogram of the non selected genes is still skewed to the left. Both the density plot and the histogram of the p-values show that the assumption of a normal distribution is violated. The @BH:1995 procedure is too conservative for this data set. 

We believe we can detect more differentiated genes using a local approach where we assume that the observed histogram represents a mixture of different Gaussian distributions, each with their own mean and standard deviance. We will start from the original z-values, the parameters of the Gaussian distributions will be estimated with maximum likelihood.


```{r localfdr}
#local fdr
fdr_x <- locfdr(t_stats$z_val,plot=4) 
prop_h0 <- fdr_x$fp0['mlest','p0']
```

The left plot shows the density of both populations. The proportion of differentiated genes is expected to be `r round(100*(1 - prop_h0),2)`%. This means `r round(10000*(1 - prop_h0),0)` genes for our data set. We do not expect to find signal genes in the right tail. 

The second plot shows the local false discovery rate. Since we want to control the FDR we are interested in the red dashed line. The height of the left FDR-graph is the FDR for all the genes with z-values smaller than the threshold shown on the x-axis.

```{r control_lfdr}

FDR_left <- fdr_x$mat[,'Fdrleft']
z_mat <- fdr_x$mat[,'x']
lfdr_mat <- fdr_x$mat[,'fdr']
dens1_mat <- fdr_x$mat[,'p1f1'] 

id <- which.max(FDR_left[FDR_left < fdr_level])
t_int <- (fdr_level - FDR_left[id])/(FDR_left[id+1] - FDR_left[id])
threshold <- z_mat[id] * (1-t_int) + z_mat[id]*t_int 
lfdr_level <- lfdr_mat[id] * (1-t_int) + lfdr_mat[id]*t_int
cdf_true_positive = sum(dens1_mat[1:id])+ dens1_mat[id+1]* t_int
prop_tp_r <- cdf_true_positive/sum(dens1_mat)
```

We want to control the FDR for 10%, for this we can find the value on the x-axis for which the left FDR - graph is equal to 10% (or calculate the threshold using interpolation), the threshold for the z-values becomes `r round(threshold,3)`.

The third graph returns the probability that a non-null gene can be detected when the nominal local fdr is set at a given local fdr-level. The local fdr-level associated with an FDR of 10% is `r round(100* lfdr_level,1)`% which means that we expect `r round(100*prop_tp_r)`% of all differentiated genes to be correctly identified.   

```{r lfdr_genes}

t_stats <- t_stats %>%
  mutate(
    lfdr = fdr_x$fdr,
    zfdr = (lfdr < lfdr_level) * z_val,
    isdetected_lfdr = factor(lfdr < lfdr_level,
                        levels= c(TRUE,FALSE),
                        labels = c('detected','H0 not rejected')))
summ_lfdr <- t_stats %>%
  filter(lfdr < lfdr_level) %>%
  summarize(nr_of_genes=n(), mean_lfdr=mean(lfdr))

ggplot(t_stats) + 
  scale_fill_manual(values=c("firebrick","lightblue")) +
  geom_histogram(mapping=aes(x = stat, fill=isdetected_lfdr),
                 bins = 20,alpha = .7)+
  labs(title= 't-statistics for differentiated genes Local FDR')+
  theme_bw()       
lfdr_genes <- t_stats %>% filter(lfdr < lfdr_level)%>% .$gene
```

We have found `r summ_lfdr$nr_of_genes` candidates for differentiated genes. The mean local fdr the detected genes is `r round(100*summ_lfdr$mean_lfdr,1)`%, we expect to have `r round(summ_lfdr$mean_lfdr* summ_lfdr$nr_of_genes)` falsely detected genes. The genes detected by the of the local fdr will be added as an appendix.  

### Shorter version voor hypothesis testing: only text
In this section we find out which genes are differentially expressed between the two kidney rejection groups. Since we do not know a priori whether differentiated genes will show higher or lower intensities we opt for 2 sided t-tests with unequal variances.

\[H_0: \mu_{normal}(gene) = \mu_{reject}(gene) \]
\[H_1: \mu_{normal}(gene) \neq \mu_{reject}(gene) \]

We first performed t-tests for every gene in the data, with an individual significance level of 5% (`r as.numeric(table_p_nonadj['TRUE'])`  genes). The histogram indicates that the spread of the transformed z-value is too large, and that there are enough differentiated genes present to visibly skew the distribution.

After centering and scaling the z values, we adjusted for multiple testing by controlling the FDR to 10% [@BH:1995]. This procedure lowered the number of candidate genes drastically (`r nr_detected_fdr` genes), mainly because of the scaling. The density plot still does not fit the standard normal distribution.

The local fdr seems a better approach since it allows for more than one cluster. Starting from the original z-values the parameter of each distribution is estimated with maximum likelihood. 
The red dashed line on the middle plot represents the FDR. In order to control the FDR for 10%, we find the value on the x-axis for which this height is equal to 10% (or calculate this threshold using interpolation), the threshold for the z-values becomes `r round(threshold,3)`.
The third graph returns the probability to detect a differentiated non-null gene given the local fdr-level. The local fdr-level associated with an FDR of 10% is `r round(100* lfdr_level,1 )`%, which means that we expect `r round(100*prop_tp_r)`% of all differentiated genes to be correctly identified.

The proportion of differentiated genes is expected to be `r round(100*(1-prop_h0),2)`% percent. This means `r round(10000*(1 - prop_h0),0)` genes for our data set. We have found `r summ_lfdr$nr_of_genes` candidates for differentiated genes. The mean local fdr the detected genes is `r round(100* lfdr_level,1)`%, so we expect to have `r round(summ_lfdr$mean_lfdr* summ_lfdr$nr_of_genes)` falsely detected genes. 
The genes detected by the local fdr will be added as an appendix.

## Model Selection
In the next step we want the predict the rejection status using the gene expression levels. Therefore, we first start with splitting the dataset into a train (70%) and test (30%) dataset.

Hier ook nog de testset deftig scalen? maar hoe?

```{r training and test data}
set.seed(1234)
trainset <- sample(nrow(X_raw), 0.7*nrow(X_raw))
trainX <- X_raw[trainset, ]
trainX <- scale(trainX, center = TRUE, scale = TRUE)
dim(trainX) #175*10000
trainY <- Y[trainset]

testX <- X_raw[-trainset, ]
dim(testX) #75*10000
testY <- Y[-trainset]

train_data <- data.frame(trainY, trainX)
test_data <- data.frame(testY, testX)
```
Here, we will evaluate three prediction models: principal component regression (PCR), Ridge regression and Lasso regression.

# Principal compentent regression (PCR)

```{r PCR}
# Calculate PCA and extract scores
pca_X <- prcomp(trainX)
Z <- pca_X$x

## Total number of available PCs
n_PC <- ncol(Z)
n_PC
```
First, the principal components are calculated and the total number of available PC's is selected. In our case, `r n_PC` PC's are available.

```{r fullPCR, cache=TRUE}
fit_data <- data.frame(trainY, Z)

## Example of PC Log. Reg. with all PCs
full_model <- suppressMessages(glm(trainY ~ ., data = fit_data, family = "binomial"))

set.seed(1234)
full_model_cv <- suppressMessages(cv.glm(
  data = fit_data,  glmfit = full_model,
  cost = auc, K = 10  # note: specify the auc function (from pROC) without`()`!
))

## We'll just use the raw one here
full_model_cv$delta[1] # This is the AUC for this particular model estimated by AUC
```
First we try to fit a model with all the PCs, using crossvalidation. Via crossvalidation will the training set be divived into (hoeveel we er kiezen) approximately equal subsets. This means we will have overfitting, as the model will predict the training dataset exactly, but this will not be reproducible to other (or the test) dataset. Here we get an area under the curve (AUC) of `r full_model_cv$delta[1]`.

```{r eachPC, cache=TRUE}
## Now we'll wrap this code in a for-loop and repeat for each number of PCs
cv_auc <- vector("numeric", length = n_PC)
set.seed(1234) # seed for reproducibility
for (i in seq_len(n_PC)) {
  ## Prepare fit_data; subset number of PCs to i
  fit_data <- data.frame(trainY, Z[, 1:i, drop = FALSE])  # use drop = FALSE to avoid problems when subsetting single column
  pcr_mod <- suppressMessages(
    glm(trainY ~ ., data = fit_data, family = "binomial")
  )
  
  ## Do 4-fold CV while suppressing Warnings and Messages
  cv <- suppressMessages(
      cv.glm(fit_data, pcr_mod, cost = pROC::auc, K = 4)
            )
  cv_auc[i] <- cv$delta[1]
}

#als ik hier een k=10 gebruik, krijg ik de Error in roc.default(response, predictor, auc = TRUE, ...) : 'response' must have two levels
#=> niet in alle CV trainingsets zit dan de 2 rejection status?
names(cv_auc) <- seq_along(cv_auc) 

cv_auc

## Finding the optimal nr. of PCs corresponds to finding the max. AUC
optim_nPC <- names(which.max(cv_auc))
optim_nPC
cv_auc[optim_nPC]

plot(names(cv_auc), cv_auc, xlab = "n PCs", ylab = "AUC", type = "l")
abline(v = optim_nPC, col = "red")
```

In a second step, we loop and repeat this for each PC. This gives us the optimal number of PC's of `r optim_nPC`, which corresponds to an AUC of `cv_auc[optim_nPC]`.

```{r PCRfinal, cache=TRUE}
pca <- prcomp(trainX)
Vk <- pca$rotation[, 1:optim_nPC] # the loadings matrix
Zk <- pca$x[, 1:optim_nPC]

pcr_model1 <- glm(trainY ~ Zk, family = "binomial")
summary(pcr_model1)
suppressMessages(plot(pcr_model1, plottype = "validation"))
```
In the last step, a principle component egression model with `r optim_nPC` is fitted. The Akaike's Information Criterion (AIC) for this model is 158.43.

# Ridge regression

# Lasso regression

## Model Evaluation

## Conclusions


## References
<div id="refs"></div>

## Appendix: Names of candidate differentiated genes
`r lfdr_genes`
