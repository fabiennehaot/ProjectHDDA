---
title: "HDDA Project"
author: "Lisa Barbier, Klara Dewitte, Fabienne Haot, Kasimir Putseys"
date: "December 2021"
bibliography: references.bib
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(gridExtra)
library(HDDAData)
library(glmnet)
library(MASS)
library(locfdr)
library(boot)
library(pROC)
```

# Research Question

# Executive Summary

# Technical Report

## Data

Data description in [@einecke:2010]
```{r getdata}
data("Einecke2010Kidney")
X_raw <- Einecke2010Kidney[, -1]
<<<<<<< HEAD
Y <- factor(Einecke2010Kidney[, 1], 
            levels = c(0,1),labels = c('accept','reject'))
=======
Y <- factor(Einecke2010Kidney[, 1], levels = c(0, 1),labels = c('accept', 'reject'))
>>>>>>> a62b877bd2ca1b45d534b311c9c01ab4f3ba8da1
```

## Exploration of the Data
The data exploration is started by examining the Einecke 2010 kidney data. Of the 250 observations, there are 67 cases where the kidney transplant was rejected and 183 where it was accepted. Furthermore, no missing values were detected. Next, we look at possible outliers. The following plot illustrates that the data seem to be calibrates, close to centered but not scaled. Finally, no outliers were found.

```{r, echo=FALSE}
#missing values
nr_values  <- sapply(X_raw, function(var) {length(var)})
nr_missing <- sapply(X_raw, function(var) {length(which(is.na(var)))})
nr_unique  <- sapply(X_raw, function(var) {n_distinct(var)})

#outliers
max_per_var <- sapply(Einecke2010Kidney, max)
min_per_var <- sapply(Einecke2010Kidney, min)
mean_per_var <- sapply(Einecke2010Kidney, mean)
tb_summary <- tibble(Name= colnames(Einecke2010Kidney),
                     Nr = seq_along(Einecke2010Kidney),
                     MaxPerVar = max_per_var,
                     MinPerVar = min_per_var,
                     MeanPerVar=mean_per_var)
ggplot(data=tb_summary)+
  geom_point(mapping=aes(x= Nr,y=MinPerVar),col='blue',size=.3)+
  geom_point(mapping=aes(x= Nr,y=MeanPerVar),col='green',size=.3)+
  geom_point(mapping=aes(x= Nr,y=MaxPerVar),col='red',size=.3)+
  theme_bw()

X <- scale(X_raw)

```
To get a better idea of the data, we plot the observations on the first two genes. Here, we don’t see a separation of kidney acceptance and therefore, in the next step we execute PCA to get more insight. 

```{r}
Einecke2010Kidney$rejection_fac <- as.factor(ifelse(Einecke2010Kidney$Reject_Status == 0, "Accepted", "Rejected"))
ggplot(Einecke2010Kidney) +
  geom_point(aes(X211352_s_at, X239275_at, col = rejection_fac)) +
  ggtitle("Visualizing the data on the first 2 genes")

```
To get a better idea of the data, we plot the observations on the first two genes. Here, we don’t see a separation of kidney acceptance and therefore, in the next step we execute PCA to get more insight. 

```{r pressure, echo=FALSE}
#PCA
svd_x <- svd(X)
Z <- svd_x$u %*% diag(svd_x$d) # Calculate the scores
V <- svd_x$v                 # Calculate the loadings
pca_x <- prcomp(X, center = FALSE, scale. = FALSE)

par(pch = 19, mfrow = c(1, 2))
plot(svd_x$d, type = "b", ylab = "Singular values", xlab = "PCs")

var_explained <- svd_x$d^2 / sum(svd_x$d^2)
plot(var_explained,
     type = "b", ylab = "Percent variance explained", xlab = "PCs",
     col = 2
)
```
The first 2 PC’s explain 19% of the variability of X and with the first 50 PC’s we obtain 63% of the variability of PC’s. 

To get more insight in the research question, we examine whether kidney rejection is associated the first two PC’s, the following plot illustrates this. 
```{r}
Scores <- X %*% svd_x$v
tb_scores <- tibble(PCA1 = Scores[,1],
                    PCA2 = Scores[,2],
                    Object=rownames(Scores),
                    Rejected = as.factor(Y))

ggplot(tb_scores)+
  geom_point(mapping=aes(x= PCA1,y=PCA2, col=Rejected))+
  labs(title="Scores for 2 PCA's")+
  theme_bw()  
```
We see the PC’s do not clearly separate the accepted and rejected cases. However, we do see that the rejected cases are more present in the bottom left corner and the accepted cases in top right corner.

Next, it is interesting to get more information on which genes are driving the PC’s. A lot of the PC’s are very small (near zero) and therefore, we create a histogram of the loadings to get a better visualization of these loadings.

```{r}
V[1:20, 1]
V[1:20, 2]



```

```{r}
hist(V[, 1], breaks = 50, xlab = "PC 1 loadings", main = "")
abline(v = c(
  quantile(V[, 1], 0.05),
  quantile(V[, 1], 0.95)
), col = "red", lwd = 2)
```

```{r}
hist(V[, 2], breaks = 50, xlab = "PC 2 loadings", main = "")
abline(v = c(
  quantile(V[, 2], 0.05),
  quantile(V[, 2], 0.95)
), col = "red", lwd = 2)

```
The histograms confirm this which gives us a reason to believe that sparse PCA is worthwhile. 

```{r}
set.seed(45)
fit_loadings1 <- cv.glmnet(X, Z[, 1],
                           alpha = 0.5, nfolds = 5)
set.seed(45)
fit_loadings2 <- cv.glmnet(X, Z[, 2], alpha = 0.5, nfolds = 5)

par(mfrow = c(1, 1))
plot(fit_loadings1$glmnet.fit, main = "PC1", xvar = "lambda")
abline(v = log(fit_loadings1$lambda.min), lty = 3)
abline(v = log(fit_loadings1$lambda.1se), lty = 3)
plot(fit_loadings2$glmnet.fit, main = "PC2", xvar = "lambda")
abline(v = log(fit_loadings2$lambda.min), lty = 3)
abline(v = log(fit_loadings2$lambda.1se), lty = 3)
```
```{r}
fit_loadings1
fit_loadings2

```

```{r}
sparse_loadings1 <- as.vector(coef(fit_loadings1, s = fit_loadings1$lambda.1se))
sparse_loadings2 <- as.vector(coef(fit_loadings2, s = fit_loadings2$lambda.1se))
## How many non-zero loadings do we have (excluding the intercept)?
(non_zero1 <- sum(abs(sparse_loadings1[-1]) > 0))
(non_zero2 <- sum(abs(sparse_loadings2[-1]) > 0))

SPC1 <- X %*% sparse_loadings1[-1] # without the intercept
SPC2 <- X %*% sparse_loadings2[-1] # without the intercept
```
For both PC1 and PC2 around 200 genes are important.

```{r}
par(mfrow = c(1, 2))
cols <- c("0" = "red", "1" = "blue")
plot(Z[, 1], Z[, 2],
     col = cols[Y], xlab = "PC1", ylab = "PC2", pch = 16,
     main = "All genes \nfor PC1 and PC2"
)
plot(SPC1, SPC2,
     col = cols[Y], xlab = "SPC1", ylab = "SPC2", pch = 16,
     main = paste(non_zero1, "genes for SPC1 \n and", non_zero2, "genes for SPC2")
)
```

We clearly can can see that with this subset of genes the same result can be obtained.

Finally, because PCA only takes the genes into account in the decomposition and not the class membership, we executed LDA. When executing the LDA in R we achieved the warning that the variables are collinear, which is confirmed by the sparse PCA. 
```{r, warning=FALSE}
kidney.lda <- MASS::lda(x = X, grouping = Y)
Vlda <- kidney.lda$scaling
colnames(Vlda) <- paste0("V",1:ncol(Vlda))
Zlda <- X%*%Vlda
colnames(Zlda) <- paste0("Z",1:ncol(Zlda))
par(mfrow = c(1, 1))
boxplot(Zlda ~ Y, col = cols, ylab = expression("Z"[1]),
        main = "Separation of accepted and rejected samples by LDA")
```

The LDA separates the accepted case (red) and the rejected case (blue) fairly well. However, it might be worthwhile to execute a sparse LDA due to the collinearity between thhe genes. The result of this is illustrated in the next figure.

```{r}
set.seed(45)
lda_loadings <- cv.glmnet(X, Zlda, alpha = 0.5, nfolds = 5)
plot(lda_loadings)

sparse_lda_loadings <- as.vector(
  coef(lda_loadings, s = lda_loadings$lambda.1se)
)

SLDA <- X %*% sparse_lda_loadings[-1]

# number of non-zero loadings
n_nonzero <- sum(sparse_lda_loadings != 0)
boxplot(SLDA ~ Y,
        col = cols, ylab = "SLDA",
        main = sprintf("Subset of %d genes", n_nonzero)
)
```
We see that the accepted cases are situated just below 0 and the rejected cases around 1.
## Hypothesis Testing 


Not every gene in the data set is an indicator for kidney rejection. In this section we find out which genes are differentially expressed between the two kidney rejection groups. Since we do not know a priori whether differentiated genes will show higher or lower intensities we opt for 2 sided t-tests with unequal variances. 

\[H_0: \mu_{normal}(gene) = \mu_{reject}(gene) \]
\[H_1: \mu_{normal}(gene) \neq \mu_{reject}(gene) \]

We first perform these t-tests for every gene in the data, with an individual significance level of 5%. Because we are performing multiple testing we would expect to find about 500 genes with a p-value lower than 5% if the null hypothesis holds for all the genes. 

```{r hyptest_unadjusted, fig.height= 4}
gene_data <- as.matrix(X_raw)
group <- Y
# non adjusted p-values as named vector
significance_level <- 0.05
ttest_results <- apply(gene_data, 2, function(x) {
  t_test <- t.test(x ~ group)
  p_val <- t_test$p.value
  stat <- t_test$statistic
  df <- t_test$parameter
  z_val <- case_when(stat < 0 ~ qnorm(p_val/2), TRUE~ qnorm(1-p_val/2))
  tibble(stat = stat, p_val = p_val,z_val = z_val, df = df)})
t_stats <- bind_rows(ttest_results) %>%
  mutate(gene = colnames(X_raw),
         isdetected_t = factor(p_val < significance_level,
                         levels=c(TRUE, FALSE),
                         labels = c('detected','H0 not rejected')))

# plot histogram for the unadjusted z-values
mean_z <- round(mean(t_stats$z_val),3) 
sd_z <- round(sd(t_stats$z_val),3) 

ggplot(data=t_stats) + 
  geom_histogram(mapping=aes(x = z_val,fill=isdetected_t),
                 bins=20,alpha = .7)+
  scale_fill_manual(values=c("firebrick","lightblue")) +
  labs(title = "multiple t-tests",
       subtitle="histogram of transformed z-values",
       x = "z-values")+
  annotate(geom="text", x=-8, y=1500, 
           label=paste0("mean: ", mean_z,"\nstdev:", sd_z))+

  theme_bw()

# number and names of differentiated genes
table_p_nonadj <- table(t_stats$p_val < 0.05)
nonadj_diff_genes <- t_stats %>% filter(p_val<.05) %>% .$gene

```

There are `r as.numeric(table_p_nonadj['TRUE'])` out of 10000 genes with unadjusted p-value below 0.05, while we expected to find 500 genes if the null hypothesis were true for all the genes.  

The histogram does not show a standard normal distribution as the observations are skewed to the left. We can derive from this histogram that the differentiated genes tend to have higher intensities in the group with rejected kidneys. There is also reason to assume that the non-differentiated genes give rise to a null distribution which is not standard normal.


```{r hyptest_fdr_hist}
#fdr analysis - current behaviour of p-values: uniform?
t_stats %>%
  ggplot(aes(x = p_val)) +
  geom_histogram(fill = "firebrick",breaks = seq(0,1,.05),alpha=.7)+
  labs(title = 'histogram of p-values')+
  theme_bw()
```

This histogram should show a distribution which is close to a uniform distribution
for the larger p-values, but with higher frequencies for the smallest p-values.
The higher number of genes with small p-values indicate in that situation that some genes show a different behaviour regarding kidney rejection. 

We see a more of less constant frequency for p-values higher than 35%, we also see a clear peak for p-values lower than 5% but the frequencies for values between 5% and 35% are not constant. The assumption of a standard normal null distribution is not valid. Because of this we underestimate the lower tail of the non differentiated genes with respect to the rejection status. We will scale the z values to a standard normal distribution and recalculate the associated p-values. 

We will also adjust for multiple testing by controlling the false detection ratio (FDR = 10%): the list with detected genes will contain false positives, but the ratio of the truly differentiated detected genes and all the detected genes is expected to be 90%. The p-values are adjusted using the Benjamini and Hochberg[@BH:1995] procedure. 

```{r hyptest_fdr_padj}
# adjusted p values
fdr_level <- 0.1
t_stats <- t_stats %>%
  mutate(z_scale = (z_val - mean(z_val))/sd(z_val),
         p_scale = pnorm(z_scale),
         p_adj = p.adjust(p_scale, method="fdr"),
         isdetected_fdr = factor(p_adj< fdr_level,
                         levels=c(TRUE,FALSE),
                         labels = c('detected','H0 not rejected')))

# monotonous transformation:
pp_adj <- t_stats %>%
  ggplot(aes(x=p_scale,y=p_adj)) +
  geom_point(size = .3,color='red') +
  geom_segment(x=0,y=0,xend=1,yend=1) +
  labs(y= "adjusted p-value (BH, 1995)") +
  theme_bw()

# right plot removes observations with high fdr 
suppressWarnings(
  grid.arrange(pp_adj,
             pp_adj + ylim(c(0,fdr_level)),
             ncol=2))

# these are the detected genes and a histogram of their t-values
table_p_adj <- table(t_stats$p_adj < fdr_level)
nr_detected_fdr <- as.numeric(table_p_adj['TRUE'])
fdr_genes <- t_stats %>% filter(p_adj < fdr_level)%>% .$gene
ggplot(t_stats) + 
  scale_fill_manual(values=c("firebrick","lightblue")) +
  geom_histogram(mapping=aes(x = stat, fill=isdetected_fdr), 
                 bins = 20,alpha=.7)+
  labs(title= 't-statistics for differentiated genes FDR',
       subtitle = ' p-values, rescaled and adjusted for multiple testing')+
  theme_bw()            
  
```

The scaling has lowerd the number of candidate genes drastically.
There are now `r nr_detected_fdr` out of 10000 genes with adjusted p-value below 10%. We still expect `r round(nr_detected_fdr/10,-1)` of these genes to be false detections. 

All of the detected genes are found in the lower tail, which seems acceptable. The histogram of the non selected genes is still skewed to the left however. The histogram also gives the impression that we should have gotten more differenciated genes.

We will try to make a better list using a local approach where we assume that the observed histogram represents a mixture of different Gaussian distributions, each with their own mean and standard deviance. We will use the z-values before scaling for this part. The means and standard deviance of the different distributions will be calculated using maximum likelihood.



```{r localfdr}
#local fdr
fdr_x <- locfdr(t_stats$z_val,plot=4) 
prop_h0 <- fdr_x$fp0['mlest','p0']
```

The left plot shows the density of both population. The proportion of differentiated genes is expected to be`r round(100*(1 - prop_h0),2)` percent. This means `r round(10000*(1 - prop_h0),0)`genes for our data set. The blue line represents the null distribution, the purple bars represent the differentiated genes. We do not expect to find signal genes with high t-values. 

The second plot shows the local false discovery rate. Since we want to control the FDR we are more interested in the red dashed line, which represents the left FDR-graph with the FDR for all the genes in the left tail of the empirical distribution. The Efdr represents the expected local fdr for a typical non-null feature, i.e. the mean local fdr, weighted with the density represented by all the purple bars.


```{r control_lfdr}

FDR_left <- fdr_x$mat[,'Fdrleft']
z_mat <- fdr_x$mat[,'x']
lfdr_mat <- fdr_x$mat[,'fdr']
dens1_mat <- fdr_x$mat[,'p1f1'] 

id <- which.max(FDR_left[FDR_left < fdr_level])
t_int <- (fdr_level - FDR_left[id])/(FDR_left[id+1] - FDR_left[id])
threshold <- z_mat[id] * (1-t_int) + z_mat[id]*t_int 
lfdr_level <- lfdr_mat[id] * (1-t_int) + lfdr_mat[id]*t_int
cdf_true_positive = sum(dens1_mat[1:id])+ dens1_mat[id+1]* t_int
prop_tp_r <- cdf_true_positive/sum(dens1_mat)
```

The threshold for a local fdr of 20% is `r fdr_x$z.2[1]`. If we want to control the FDR for 10%, we have to find the value on the x-axis for which the left FDR - graph is equal to 10%. The threshold for the z-values using interpolation on the points of the left FDR-graph becomes: `r round(threshold,3)`.

The third graph returns the probability that a non-null gene can be detected when the nominal local fdr is set at a given local fdr-level. The local fdr-level associated with an FDR of 10% is `round(100* lfdr_level,1)` percent which means that we expect `r round(100*prop_tp_r)` percent of all differentiated genes to be correctly identified.   

```{r lfdr_genes}

t_stats <- t_stats %>%
  mutate(
    lfdr = fdr_x$fdr,
    zfdr = (lfdr < lfdr_level) * z_val,
    isdetected_lfdr = factor(lfdr < lfdr_level,
                        levels= c(TRUE,FALSE),
                        labels = c('detected','H0 not rejected')))
summ_lfdr <- t_stats %>%
  filter(lfdr < lfdr_level) %>%
  summarize(nr_of_genes=n(), mean_lfdr=mean(lfdr))

ggplot(t_stats) + 
  scale_fill_manual(values=c("firebrick","lightblue")) +
  geom_histogram(mapping=aes(x = stat, fill=isdetected_lfdr),
                 bins = 20,alpha = .7)+
  labs(title= 't-statistics for differentiated genes Local FDR')+
  theme_bw()       
lfdr_genes <- t_stats %>% filter(lfdr < lfdr_level)%>% .$gene
```

We have found `r summ_lfdr$nr_of_genes` candidates for differentiated genes. The mean local fdr the detected genes is `r round(100*summ_lfdr$mean_lfdr,1)` percent, which means we expect to have `r round(summ_lfdr$mean_lfdr* summ_lfdr$nr_of_genes)` falsely detected genes. The genes detected by the of the local fdr will be added as an appendix.  

## Model Selection
In the next step we want the predict the rejection status using the gene expression levels. Therefore, we first start with splitting the dataset into a train (70%) and test (30%) dataset.

Hier ook nog de testset deftig scalen? maar hoe?

```{r training and test data}
set.seed(1234)
trainset <- sample(nrow(X_raw), 0.7*nrow(X_raw))
trainX <- X_raw[trainset, ]
trainX <- scale(trainX, center = TRUE, scale = TRUE)
scale_factor <- attr(trainX,"scaled:scale")
scale_translation <- attr(trainX,"scaled:center")
trainX <- as.data.frame(trainX)

dim(trainX) #175*10000
trainY <- Y[trainset]
testX <- X_raw[-trainset, ]
dim(testX) #75*10000
testY <- Y[-trainset]

for (varnr in seq_along(testX)){
    xbar <- scale_translation[varnr]
    xsd <- scale_factor[varnr]
    testX[[varnr]] <- (testX[[varnr]]-xbar )/xsd}
    
list(tr_x = trainX, tr_y = trainY, test_x = testX, test_y = testY)

train_data <- data.frame(trainY, trainX)
test_data <- data.frame(testY, testX)
```
Here, we will evaluate three prediction models: principal component regression (PCR), Ridge regression and Lasso regression.

# Principal compentent regression (PCR)

```{r PCR}
# Calculate PCA and extract scores
pca_X <- prcomp(trainX)
Z <- pca_X$x

## Total number of available PCs
n_PC <- ncol(Z)
n_PC
```
First, the principal components are calculated and the total number of available PC's is selected. In our case, `r n_PC` PC's are available.

```{r full PCR, cache=TRUE, warning=FALSE}
fit_data <- data.frame(trainY, Z)

## Example of PC Log. Reg. with all PCs
full_model <- suppressMessages(glm(trainY ~ ., data = fit_data, family = "binomial"))

set.seed(1234)
full_model_cv <- suppressMessages(cv.glm(
  data = fit_data,  glmfit = full_model,
  cost = auc, K = 10  # note: specify the auc function (from pROC) without`()`!
))

## We'll just use the raw one here
full_model_cv$delta[1] # This is the AUC for this particular model estimated by AUC
```
First we try to fit a model with all the PCs, using crossvalidation. Via crossvalidation will the training set be divived into (hoeveel we er kiezen) approximately equal subsets. This means we will have overfitting, as the model will predict the training dataset exactly, but this will not be reproducible to other (or the test) dataset. Here we get an area under the curve (AUC) of `r full_model_cv$delta[1]`.

```{r each PC, cache=TRUE, warning=FALSE}
## Now we'll wrap this code in a for-loop and repeat for each number of PCs
cv_auc <- vector("numeric", length = n_PC)
set.seed(1234) # seed for reproducibility
for (i in seq_len(n_PC)) {
  ## Prepare fit_data; subset number of PCs to i
  fit_data <- data.frame(trainY, Z[, 1:i, drop = FALSE])  # use drop = FALSE to avoid problems when subsetting single column
  pcr_mod <- suppressMessages(
    glm(trainY ~ ., data = fit_data, family = "binomial")
  )
  
  ## Do 4-fold CV while suppressing Warnings and Messages
  cv <- suppressMessages(
      cv.glm(fit_data, pcr_mod, cost = pROC::auc, K = 4)
            )
  cv_auc[i] <- cv$delta[1]
}

#als ik hier een k=10 gebruik, krijg ik de Error in roc.default(response, predictor, auc = TRUE, ...) : 'response' must have two levels
names(cv_auc) <- seq_along(cv_auc) 
cv_auc

## Finding the optimal nr. of PCs corresponds to finding the max. AUC
optim_nPC <- names(which.max(cv_auc))
optim_nPC
cv_auc[optim_nPC]

plot(names(cv_auc), cv_auc, xlab = "n PCs", ylab = "AUC", type = "l")
abline(v = optim_nPC, col = "red")
```

In a second step, we loop and repeat this for each PC. This gives us the optimal number of PC's of `r optim_nPC`, which corresponds to an AUC of `cv_auc[optim_nPC]`.

```{r PCR 2, cache=TRUE, warning=FALSE}
pca <- prcomp(trainX)
Vk <- pca$rotation[, 1:optim_nPC] # the loadings matrix
Zk <- pca$x[, 1:optim_PC]

pcr_model1 <- glm(trainY ~ Zk, family = "binomial")
summary(pcr_model1)
suppressMessages(plot(pcr_model1, plottype = "validation"))
```
In the last step, a principle component egression model with `r optim_nPC` is fitted. The Akaike's Information Criterion (AIC) for this model is 158.43.

# Ridge regression

# Lasso regression

## Model Evaluation

## Conclusions


## References
<div id="refs"></div>

## Appendix: Names of candidate differentiated genes
`r lfdr_genes`
