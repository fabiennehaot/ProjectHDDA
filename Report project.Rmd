---
title: "HDDA Project"
author: "Lisa Barbier, Klara Dewitte, Fabienne Haot, Kasimir Putseys"
date: "December 2021"
bibliography: references.bib
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(gridExtra)
library(HDDAData)
library(glmnet)
library(MASS)
library(locfdr)
library(boot)
library(pROC)
```

# Research Question

# Executive Summary

# Technical Report

## Data

Data description in [@einecke:2010]
```{r getdata}
data("Einecke2010Kidney")
X_raw <- Einecke2010Kidney[, -1]
Y <- factor(Einecke2010Kidney[, 1], 
            levels = c(0,1),labels = c('accept','reject'))
```

## Exploration of the Data
The data exploration is started by examining the Einecke 2010 kidney data. Of the 250 observations, there are 67 cases where the kidney transplant was rejected and 183 where it was accepted. Furthermore, no missing values were detected. Next, we look at possible outliers. The following plot illustrates that the data seem to be calibrates, close to centered but not scaled. Finally, no outliers were found.

```{r, echo=FALSE}
#missing values
nr_values  <- sapply(X_raw, function(var) {length(var)})
nr_missing <- sapply(X_raw, function(var) {length(which(is.na(var)))})
nr_unique  <- sapply(X_raw, function(var) {n_distinct(var)})

#outliers
max_per_var <- sapply(Einecke2010Kidney, max)
min_per_var <- sapply(Einecke2010Kidney, min)
mean_per_var <- sapply(Einecke2010Kidney, mean)
tb_summary <- tibble(Name= colnames(Einecke2010Kidney),
                     Nr = seq_along(Einecke2010Kidney),
                     MaxPerVar = max_per_var,
                     MinPerVar = min_per_var,
                     MeanPerVar=mean_per_var)
ggplot(data=tb_summary)+
  geom_point(mapping=aes(x= Nr,y=MinPerVar),col='blue',size=.3)+
  geom_point(mapping=aes(x= Nr,y=MeanPerVar),col='green',size=.3)+
  geom_point(mapping=aes(x= Nr,y=MaxPerVar),col='red',size=.3)+
  theme_bw()

X <- scale(X_raw)

```
To get a better idea of the data, we plot the observations on the first two genes. Here, we don’t see a separation of kidney acceptance and therefore, in the next step we execute PCA to get more insight. 

```{r}
Einecke2010Kidney$rejection_fac <- as.factor(ifelse(Einecke2010Kidney$Reject_Status == 0, "Accepted", "Rejected"))
ggplot(Einecke2010Kidney) +
  geom_point(aes(X211352_s_at, X239275_at, col = rejection_fac)) +
  ggtitle("Visualizing the data on the first 2 genes")

```
To get a better idea of the data, we plot the observations on the first two genes. Here, we don’t see a separation of kidney acceptance and therefore, in the next step we execute PCA to get more insight. 

```{r pressure, echo=FALSE}
#PCA
svd_x <- svd(X)
Z <- svd_x$u %*% diag(svd_x$d) # Calculate the scores
V <- svd_x$v                 # Calculate the loadings
pca_x <- prcomp(X, center = FALSE, scale. = FALSE)

par(pch = 19, mfrow = c(1, 2))
plot(svd_x$d, type = "b", ylab = "Singular values", xlab = "PCs")

var_explained <- svd_x$d^2 / sum(svd_x$d^2)
plot(var_explained,
     type = "b", ylab = "Percent variance explained", xlab = "PCs",
     col = 2
)
```
The first 2 PC’s explain 19% of the variability of X and with the first 50 PC’s we obtain 63% of the variability of PC’s. 

To get more insight in the research question, we examine whether kidney rejection is associated the first two PC’s, the following plot illustrates this. 
```{r expl_plot2d}
Scores <- X %*% svd_x$v
tb_scores <- tibble(PCA1 = Scores[,1],
                    PCA2 = Scores[,2],
                    Object=rownames(Scores),
                    Rejected = as.factor(Y))

p_scatter <- ggplot(tb_scores)+
  geom_point(mapping=aes(x= PCA1,y=PCA2, col=Rejected))+
  labs(title="Scores for 2 PC's")+
  scale_color_manual(values= c('red','blue'))+
  theme_bw()  

p_dens <- ggplot(tb_scores)+
  geom_density(mapping=aes(x= PCA1+PCA2, fill=Rejected), alpha = 0.5)+
  labs(title="Density for 2 PC's: densities")+
  scale_fill_manual(values=c('red','blue'))+
  theme_bw()  

grid.arrange(p_scatter,p_dens,ncol=2)

```
We see the PC’s do not clearly separate the accepted and rejected cases. However, we do see that the rejected cases are more present in the bottom left corner and the accepted cases in top right corner.

Next, it is interesting to get more information on which genes are driving the PC’s. A lot of the PC’s are very small (near zero) and therefore, we create a histogram of the loadings to get a better visualization of these loadings.

```{r}
V[1:20, 1]
V[1:20, 2]



```

```{r}
hist(V[, 1], breaks = 50, xlab = "PC 1 loadings", main = "")
abline(v = c(
  quantile(V[, 1], 0.05),
  quantile(V[, 1], 0.95)
), col = "red", lwd = 2)
```

```{r}
hist(V[, 2], breaks = 50, xlab = "PC 2 loadings", main = "")
abline(v = c(
  quantile(V[, 2], 0.05),
  quantile(V[, 2], 0.95)
), col = "red", lwd = 2)

```
The histograms confirm this which gives us a reason to believe that sparse PCA is worthwhile. 

```{r}
set.seed(45)
fit_loadings1 <- cv.glmnet(X, Z[, 1],
                           alpha = 0.5, nfolds = 5)
set.seed(45)
fit_loadings2 <- cv.glmnet(X, Z[, 2], alpha = 0.5, nfolds = 5)

par(mfrow = c(1, 1))
plot(fit_loadings1$glmnet.fit, main = "PC1", xvar = "lambda")
abline(v = log(fit_loadings1$lambda.min), lty = 3)
abline(v = log(fit_loadings1$lambda.1se), lty = 3)
plot(fit_loadings2$glmnet.fit, main = "PC2", xvar = "lambda")
abline(v = log(fit_loadings2$lambda.min), lty = 3)
abline(v = log(fit_loadings2$lambda.1se), lty = 3)
```
```{r}
fit_loadings1
fit_loadings2

```

```{r}
sparse_loadings1 <- as.vector(coef(fit_loadings1, s = fit_loadings1$lambda.1se))
sparse_loadings2 <- as.vector(coef(fit_loadings2, s = fit_loadings2$lambda.1se))
## How many non-zero loadings do we have (excluding the intercept)?
(non_zero1 <- sum(abs(sparse_loadings1[-1]) > 0))
(non_zero2 <- sum(abs(sparse_loadings2[-1]) > 0))

SPC1 <- X %*% sparse_loadings1[-1] # without the intercept
SPC2 <- X %*% sparse_loadings2[-1] # without the intercept
```
For both PC1 and PC2 around 200 genes are important.

```{r}
par(mfrow = c(1, 2))
cols <- c("0" = "red", "1" = "blue")
plot(Z[, 1], Z[, 2],
     col = cols[Y], xlab = "PC1", ylab = "PC2", pch = 16,
     main = "All genes \nfor PC1 and PC2"
)
plot(SPC1, SPC2,
     col = cols[Y], xlab = "SPC1", ylab = "SPC2", pch = 16,
     main = paste(non_zero1, "genes for SPC1 \n and", non_zero2, "genes for SPC2")
)
```

We clearly can can see that with this subset of genes the same result can be obtained.

Finally, because PCA only takes the genes into account in the decomposition and not the class membership, we executed LDA. When executing the LDA in R we achieved the warning that the variables are collinear, which is confirmed by the sparse PCA. 
```{r, warning=FALSE}
kidney.lda <- MASS::lda(x = X, grouping = Y)
Vlda <- kidney.lda$scaling
colnames(Vlda) <- paste0("V",1:ncol(Vlda))
Zlda <- X%*%Vlda
colnames(Zlda) <- paste0("Z",1:ncol(Zlda))
par(mfrow = c(1, 1))
boxplot(Zlda ~ Y, col = cols, ylab = expression("Z"[1]),
        main = "Separation of accepted and rejected samples by LDA")
```

The LDA separates the accepted case (red) and the rejected case (blue) fairly well. However, it might be worthwhile to execute a sparse LDA due to the collinearity between thhe genes. The result of this is illustrated in the next figure.

```{r}
set.seed(45)
lda_loadings <- cv.glmnet(X, Zlda, alpha = 0.5, nfolds = 5)
plot(lda_loadings)

sparse_lda_loadings <- as.vector(
  coef(lda_loadings, s = lda_loadings$lambda.1se)
)

SLDA <- X %*% sparse_lda_loadings[-1]

# number of non-zero loadings
n_nonzero <- sum(sparse_lda_loadings != 0)
boxplot(SLDA ~ Y,
        col = cols, ylab = "SLDA",
        main = sprintf("Subset of %d genes", n_nonzero)
)
```
We see that the accepted cases are situated just below 0 and the rejected cases around 1.
## Hypothesis Testing 
Not every gene in the data set is an indicator for kidney rejection. In this section we find out which genes behave differently between the two kidney rejection groups. Since we do not know a priori whether differentiated genes will show higher or lower intensities we opt for 2 sided t-tests with unequal variances. 

\[H_0: \mu_{normal}(gene) = \mu_{reject}(gene) \]
\[H_1: \mu_{normal}(gene) \neq \mu_{reject}(gene) \]

We first perform these t-tests for every gene in the data, with an individual significance level of 5%. Because we are performing multiple testing we would expect to find about 500 genes with a p-value lower than 5% if the null hypothesis holds for all the genes. 

```{r hyptest_unadjusted, fig.height= 4}
gene_data <- as.matrix(X_raw)
group <- Y
# non adjusted p-values 
ttest_results <- apply(gene_data, 2, function(x) {
  t_test <- t.test(x ~ group)
  p_val <- t_test$p.value
  stat <- t_test$statistic
  df <- t_test$parameter
  z_val <- case_when(stat < 0 ~ qnorm(p_val/2), TRUE~ qnorm(1-p_val/2))
  tibble(stat = stat, p_val = p_val,z_val = z_val, df = df)})
t_stats <- bind_rows(ttest_results) %>%
  mutate(gene = colnames(X_raw))

# selection
nonadj_level <- 0.05
selected_nonadj <- t_stats %>% filter(p_val< nonadj_level)

# plot histogram for the unadjusted z-values
mean_z <- round(mean(t_stats$z_val),3) 
sd_z <- round(sd(t_stats$z_val),3) 
threshold_nonadj <- selected_nonadj %>% 
  summarize(left = max(case_when(z_val > mean_z ~ -Inf,TRUE~ z_val)),
            right = min(case_when(z_val < mean_z ~ +Inf,TRUE~ z_val)))

hist_zval <- ggplot(data=t_stats) + 
  geom_histogram(
    mapping = aes(x = z_val,after_stat(density)),
    bins = 20, alpha = .7, fill='lightblue') +
  geom_vline(mapping=aes(xintercept = threshold_nonadj$left),color= 'red')+    
  geom_vline(mapping=aes(xintercept = threshold_nonadj$right),color= 'red')+
  geom_line(mapping=aes(x = z_val,y = dnorm(z_val)),color='blue')+
  labs(title = "multiple t-tests unadjusted",
       subtitle="histogram of corresponding z-values",
       x = "z-values",
       y= "probability density")+
  annotate(geom="text", x=-7, y=0.38, 
           label=paste0("mean: ", mean_z,"\nstdev:", sd_z))+
  theme_bw()

# signal histogram - current behaviour of p-values: uniform?
hist_pval <- ggplot(data = t_stats) +
  geom_histogram(mapping = aes(x = p_val),
                 fill = "firebrick", breaks = seq(0,1,.05),alpha=.7) +
  labs(title = 'histogram of p-values') +
  theme_bw()

grid.arrange(hist_zval, hist_pval, ncol=2)

# number of differentiated genes
table_p_nonadj <- table(t_stats$p_val < nonadj_level)
nr_detected_nonadj <- as.numeric(table_p_nonadj['TRUE'])

```

There are `r nr_detected_nonadj` out of 10000 genes with non-adjusted p-values below 0.05. The selected genes are mostly found in the lower tail, the negative z-values indicate higher intensities in the group with rejected kidneys.

The distribution is not standard normal as the observations are skewed to the left, the standard deviation is also much larger than 1.

The histogram of the p-values is not uniform, a clear peak is visible for p-values lower than 5%, but the frequencies for values between 5% and 35% drop too slowly, again because of the large spread of the distribution.

We will standardize the z values, recalculate the associated p-values and adjust for multiple testing by controlling the FDR to 10%. The p-values are adjusted using the @BH:1995 procedure. 

```{r hyptest_fdr_padj}
# adjusted p values
t_stats <- t_stats %>%
  mutate(z_scale = (z_val - mean(z_val))/sd(z_val),
         p_scale = pnorm(z_scale),
         p_adj = p.adjust(p_scale, method="fdr"))

# these are the detected genes and a histogram of their t-values
fdr_level <- 0.1
selected_fdr <- t_stats %>% filter(p_adj < fdr_level)

# plot histogram
threshold_fdr <- selected_fdr %>% 
  summarize(left = max(case_when(z_scale > 0~-Inf,TRUE~ z_scale)),
            right = min(case_when(z_scale < 0~+Inf,TRUE~ z_scale)))

hist_fdr <- ggplot(t_stats) + 
  geom_histogram(mapping=aes(x = z_scale, after_stat(density)),
                             fill="lightblue", bins = 20, alpha=.7)+
  geom_vline(mapping=aes(xintercept=threshold_fdr$left),color='red') +    
  geom_line(mapping = aes(x=z_scale, y= dnorm(z_scale)),color='blue') +
  labs(title = "multiple t-tests FDR",
       caption = 'histogram of transformed z-values, rescaled and adjusted for multiple testing')+
  theme_bw()            

# signal histogram - current behaviour of p-values: uniform?
hist_pscale <- ggplot(data = t_stats) +
  geom_histogram(mapping = aes(x = p_scale),
                 fill = "firebrick", breaks = seq(0,1,.05),alpha=.7) +
  labs(title = 'histogram of p-values') +
  theme_bw()

# monotonous transformation:
pp_adj <- t_stats %>%
  ggplot(aes(x = p_scale,y = p_adj)) +
  geom_point(size = .3,color='blue') +
  geom_segment(x=0,y=0,xend=1,yend=1) +
  geom_hline(yintercept = fdr_level, color='red') +
  labs(title = "Adjusting the p-values",
       y= "adjusted p-value (BH, 1995)") +
  theme_bw()

# show histogram z-values +  monotonous adjustment for the p-values
grid.arrange(hist_fdr, hist_pscale, pp_adj, ncol=3)

# number of detected genes
table_p_adj <- table(t_stats$p_adj < fdr_level)
nr_detected_fdr <- as.numeric(table_p_adj['TRUE'])

```

The scaling has lowered the number of candidate genes drastically.
There are now `r nr_detected_fdr` genes with adjusted p-value below 10%. All of the detected genes are now found in the lower tail. 

The histogram of the non selected genes is still skewed to the left. Both the density plot and the histogram of the p-values show that the assumption of a normal distribution is violated. The @BH:1995 procedure is too conservative for this data set. 

We believe we can detect more differentiated genes using a local approach where we assume that the observed histogram represents a mixture of different Gaussian distributions, each with their own mean and standard deviance. We will start from the original z-values, the parameters of the Gaussian distributions will be estimated with maximum likelihood.


```{r localfdr}
#local fdr
fdr_x <- locfdr(t_stats$z_val,plot=4) 
prop_h0 <- fdr_x$fp0['mlest','p0']
```

The left plot shows the density of both populations. The proportion of differentiated genes is expected to be `r round(100*(1 - prop_h0),2)`%. This means `r round(10000*(1 - prop_h0),0)` genes for our data set. We do not expect to find signal genes in the right tail. 

The second plot shows the local false discovery rate. Since we want to control the FDR we are interested in the red dashed line. The height of the left FDR-graph is the FDR for all the genes with z-values smaller than the threshold shown on the x-axis.

```{r control_lfdr}

FDR_left <- fdr_x$mat[,'Fdrleft']
z_mat <- fdr_x$mat[,'x']
lfdr_mat <- fdr_x$mat[,'fdr']
dens1_mat <- fdr_x$mat[,'p1f1'] 

id <- which.max(FDR_left[FDR_left < fdr_level])
t_int <- (fdr_level - FDR_left[id])/(FDR_left[id+1] - FDR_left[id])
threshold <- z_mat[id] * (1-t_int) + z_mat[id]*t_int 
lfdr_level <- lfdr_mat[id] * (1-t_int) + lfdr_mat[id]*t_int
cdf_true_positive = sum(dens1_mat[1:id])+ dens1_mat[id+1]* t_int
prop_tp_r <- cdf_true_positive/sum(dens1_mat)
```

We want to control the FDR for 10%, for this we can find the value on the x-axis for which the left FDR - graph is equal to 10% (or calculate the threshold using interpolation), the threshold for the z-values becomes `r round(threshold,3)`.

The third graph returns the probability that a non-null gene can be detected when the nominal local fdr is set at a given local fdr-level. The local fdr-level associated with an FDR of 10% is `r round(100* lfdr_level,1)`% which means that we expect `r round(100*prop_tp_r)`% of all differentiated genes to be correctly identified.   

```{r lfdr_genes}

t_stats <- t_stats %>%
  mutate(
    lfdr = fdr_x$fdr,
    zfdr = (lfdr < lfdr_level) * z_val,
    isdetected_lfdr = factor(lfdr < lfdr_level,
                        levels= c(TRUE,FALSE),
                        labels = c('detected','H0 not rejected')))
summ_lfdr <- t_stats %>%
  filter(lfdr < lfdr_level) %>%
  summarize(nr_of_genes=n(), mean_lfdr=mean(lfdr))

ggplot(t_stats) + 
  scale_fill_manual(values=c("firebrick","lightblue")) +
  geom_histogram(mapping=aes(x = stat, fill=isdetected_lfdr),
                 bins = 20,alpha = .7)+
  labs(title= 't-statistics for differentiated genes Local FDR')+
  theme_bw()       
lfdr_genes <- t_stats %>% filter(lfdr < lfdr_level)%>% .$gene
```

We have found `r summ_lfdr$nr_of_genes` candidates for differentiated genes. The mean local fdr the detected genes is `r round(100*summ_lfdr$mean_lfdr,1)`%, we expect to have `r round(summ_lfdr$mean_lfdr* summ_lfdr$nr_of_genes)` falsely detected genes. The genes detected by the of the local fdr will be added as an appendix.  

### Shorter version voor hypothesis testing: only text
In this section we find out which genes are differentially expressed between the two kidney rejection groups. Since we do not know a priori whether differentiated genes will show higher or lower intensities we opt for 2 sided t-tests with unequal variances.

\[H_0: \mu_{normal}(gene) = \mu_{reject}(gene) \]
\[H_1: \mu_{normal}(gene) \neq \mu_{reject}(gene) \]

We first performed t-tests for every gene in the data, with an individual significance level of 5% (`r as.numeric(table_p_nonadj['TRUE'])`  genes). The histogram indicates that the spread of the transformed z-value is too large, and that there are enough differentiated genes present to visibly skew the distribution.

After centering and scaling the z values, we adjusted for multiple testing by controlling the FDR to 10% [@BH:1995]. This procedure lowered the number of candidate genes drastically (`r nr_detected_fdr` genes), mainly because of the scaling. The density plot still does not fit the standard normal distribution.

The local fdr seems a better approach since it allows for more than one cluster. Starting from the original z-values the parameter of each distribution is estimated with maximum likelihood. 
The red dashed line on the middle plot represents the FDR. In order to control the FDR for 10%, we find the value on the x-axis for which this height is equal to 10% (or calculate this threshold using interpolation), the threshold for the z-values becomes `r round(threshold,3)`.
The third graph returns the probability to detect a differentiated non-null gene given the local fdr-level. The local fdr-level associated with an FDR of 10% is `r round(100* lfdr_level,1 )`%, which means that we expect `r round(100*prop_tp_r)`% of all differentiated genes to be correctly identified.

The proportion of differentiated genes is expected to be `r round(100*(1-prop_h0),2)`% percent. This means `r round(10000*(1 - prop_h0),0)` genes for our data set. We have found `r summ_lfdr$nr_of_genes` candidates for differentiated genes. The mean local fdr the detected genes is `r round(100* lfdr_level,1)`%, so we expect to have `r round(summ_lfdr$mean_lfdr* summ_lfdr$nr_of_genes)` falsely detected genes. 
The genes detected by the local fdr will be added as an appendix.

## Model Selection
In the next step we want the predict the rejection status using the gene expression levels. Therefore, we first start with splitting the dataset into a train (70%) and test (30%) dataset. The training set will be used to train the model and tune the hyperparameters (for example the penalty parameter in lasso regression), while the test set will be used to evaluate the out-of-sample performance of our final model.

```{r training_and_test_data, include = FALSE}
set.seed(1234)
trainset <- sample(nrow(X_raw), 0.7*nrow(X_raw))
trainX <- X_raw[trainset, ]
trainX <- scale(trainX, center = TRUE, scale = TRUE)
scale_factor <- attr(trainX,"scaled:scale")
scale_translation <- attr(trainX,"scaled:center")
trainX <- as.data.frame(trainX)

dim(trainX) #175*10000
trainY <- Y[trainset]
testX <- X_raw[-trainset, ]
dim(testX) #75*10000
testY <- Y[-trainset]

for (varnr in seq_along(testX)){
    xbar <- scale_translation[varnr]
    xsd <- scale_factor[varnr]
    testX[[varnr]] <- (testX[[varnr]]-xbar )/xsd}
    
list(tr_x = trainX, tr_y = trainY, test_x = testX, test_y = testY)

train_data <- data.frame(trainY, trainX)
test_data <- data.frame(testY, testX)
```
Here, we will evaluate three prediction models: principal component regression (PCR), Ridge regression and Lasso regression.

# Principal compentent regression (PCR)

```{r PCR}
# Calculate PCA and extract scores
pca_X <- prcomp(trainX)
Z <- pca_X$x

## Total number of available PCs
n_PC <- ncol(Z)
n_PC
```
First, the principal components are calculated and the total number of available PC's is selected. In our case, `r n_PC` PC's are available.

```{r fullPCR, cache=TRUE}
fit_data <- data.frame(trainY, Z)

## Example of PC Log. Reg. with all PCs
full_model <- suppressMessages(glm(trainY ~ ., data = fit_data, family = "binomial"))

set.seed(1234)
full_model_cv <- suppressMessages(cv.glm(
  data = fit_data,  glmfit = full_model,
  cost = auc, K = 10  # note: specify the auc function (from pROC) without`()`!
))

## We'll just use the raw one here
full_model_cv$delta[1] # This is the AUC for this particular model estimated by AUC
```
First we try to fit a model with all the PCs, using crossvalidation. Via crossvalidation will the training set be divived into (hoeveel we er kiezen) approximately equal subsets. This means we will have overfitting, as the model will predict the training dataset exactly, but this will not be reproducible to other (or the test) dataset. Here we get an area under the curve (AUC) of `r full_model_cv$delta[1]`.

```{r eachPC, cache=TRUE}
## Now we'll wrap this code in a for-loop and repeat for each number of PCs
cv_auc <- vector("numeric", length = n_PC)
set.seed(1234) # seed for reproducibility
for (i in seq_len(n_PC)) {
  ## Prepare fit_data; subset number of PCs to i
  fit_data <- data.frame(trainY, Z[, 1:i, drop = FALSE])  # use drop = FALSE to avoid problems when subsetting single column
  pcr_mod <- suppressMessages(
    glm(trainY ~ ., data = fit_data, family = "binomial")
  )
  
  ## Do 4-fold CV while suppressing Warnings and Messages
  cv <- suppressMessages(
      cv.glm(fit_data, pcr_mod, cost = pROC::auc, K = 4)
            )
  cv_auc[i] <- cv$delta[1]
}

#als ik hier een k=10 gebruik, krijg ik de Error in roc.default(response, predictor, auc = TRUE, ...) : 'response' must have two levels

names(cv_auc) <- seq_along(cv_auc) 

cv_auc

## Finding the optimal nr. of PCs corresponds to finding the max. AUC
optim_nPC <- names(which.max(cv_auc))
optim_nPC
cv_auc[optim_nPC]

plot(names(cv_auc), cv_auc, xlab = "n PCs", ylab = "AUC", type = "l")
abline(v = optim_nPC, col = "red")
```

In a second step, we loop and repeat this for each PC. This gives us the optimal number of PC's of `r optim_nPC`, which corresponds to an AUC of `cv_auc[optim_nPC]`.

```{r PCRfinal, cache=TRUE}

pca <- prcomp(trainX)
Vk <- pca$rotation[, 1:optim_nPC] # the loadings matrix
Zk <- pca$x[, 1:optim_nPC]

pcr_model1 <- glm(trainY ~ Zk, family = "binomial")
summary(pcr_model1)
suppressMessages(plot(pcr_model1, plottype = "validation"))
```
In the last step, a principle component egression model with `r optim_nPC` is fitted. The Akaike's Information Criterion (AIC) for this model is 158.43.

# Lasso regression

We are now going to use two penalised (logistic) regression models. We start with Lasso regression that uses the $L_1$-norm. We will again only use the training set to fit the model. The following figure nicely shows that when the penality parameter $\lambda$ increases the estimates are shrunken towards zero. When it hits zero, it remains zero. Hence, choosing $\lambda$ is related to feature selection.


```{r, echo=FALSE,fig.show="hold", out.width="50%"}
# Lasso
mLasso <- glmnet(x = trainX, y = trainY, alpha = 1, family  = "binomial")  # alpha = 1 -> Lasso
plot(mLasso, xvar = "lambda")

# cross validation to find optimal lambda
set.seed(44)
K <- 10 #number of folds
mCvLasso <- cv.glmnet(x = as.matrix(trainX), y = as.matrix(trainY), alpha = 1, family  = "binomial", type.measure = "auc", nfolds = K)
plot(mCvLasso)
```
To make a decision about $\lambda$, we will use cross-validation on the training data set. We use 10-fold cross-validation because our dataset is quite small, so it should not take too much time. We optimize the area under the receiver characteristic curve (AUC).

Two vertical lines are added on the figure. The first one gives the $\log(\lambda)$ for which we find the highest AUC. We will go on with this value, because with the other choice for $\lambda$, almost no genes are used anymore. We are left with 12 predictors from the original 10,000 genes (sparse solution). The values of the corresponding $\beta$ coefficients are shown on the following plot.

```{r, echo=FALSE,out.width="50%"}
# Lasso with optimal lambda
lambda_cv <- mCvLasso$lambda.min # optimal lambda
id_lambda_cv <- which(mCvLasso$lambda == lambda_cv)
auc_lasso <- mCvLasso$cvm[id_lambda_cv]

# Optimal model 
mLassoOpt <- glmnet(x = as.matrix(trainX), y = as.matrix(trainY), alpha = 1, family  = "binomial", lambda = lambda_cv)
#summary(coef(mLassoOpt))

# with optimal lambda (largest auc) the output shows the non-zero estimated regression coefficients
library(ggplot2)
qplot(summary(coef(mLassoOpt))[-1,1],
      summary(coef(mLassoOpt))[-1,3]) + xlab("gene") + ylab("beta-hat") + geom_hline(yintercept = 0, color = "red")


```

We have now our optimal Lasso model with AUC 0.82.

# Rigde regression

The last model that we will fit on the training data is the Ridge regression model that uses the $L_2$-norm. The major difference between Lasso and ridge regression is that ridge regression does not perform feature selection. We will again fit the model by using cross validation with 10 folds. The optimal $\lambda$ is much bigger compared to the Lasso regression model. The AUC value equals. 0.78.

```{r, echo=FALSE, cache = TRUE, out.width="50%"}
# Rigde regression
mRidge <- glmnet(x = as.matrix(trainX), y = as.matrix(trainY), alpha = 0, family  = "binomial")
plot(mRidge, xvar = "lambda")

# Rigde regression with cross-validation
set.seed(44)
mCvRidge <- cv.glmnet(x = as.matrix(trainX), y = as.matrix(trainY), alpha = 0, family  = "binomial", type.measure =               "auc", nfolds=K) # alp = 0 -> ridge
#plot(mCvRidge)

# Ridge regression with optimal lambda
rlambda_cv <- mCvRidge$lambda.min # optimal lambda
id_rlambda_cv <- which(mCvRidge$lambda == rlambda_cv)
auc_ridge <- mCvRidge$cvm[id_rlambda_cv]
mRidgeOpt <- glmnet(x = as.matrix(trainX), y = as.matrix(trainY), alpha = 0, family  = "binomial", lambda = rlambda_cv)
#summary(coef(mRidgeOpt))
mCvRidge$lambda.min
auc_ridge

```

## Model evaluation

We decide to continue with the optimal Lasso regression model because we found an high AUC value and because Lasso regression is easier to interpret as it does feature selection.
We now want to evaluate this final model on our test set. We can first look at the corresponding ROC curve.

```{r help, echo=FALSE,  out.width="50%", warning=FALSE}
# plot roc curve for Lasso
#BiocManager::install("plotROC")
testX <- X_raw[-trainset, ]
testY <- Y[-trainset]
library(plotROC)
library(ggplot2)
dfLassoOpt <- data.frame(
  pred = predict(mCvLasso, s = lambda_cv, newx = as.matrix(testX), family="binomial", type = "response") %>% c(.),
  known.truth = testY)

roc <-
  dfLassoOpt  %>%
  ggplot(aes(d = known.truth, m = pred)) +
  geom_roc(n.cuts = 0) +
  xlab("1-specificity (FPR)") +
  ylab("sensitivity (TPR)")
roc

calc_auc(roc)
```

The next step is to determine a good threshold $c$ as a prediction cut-off. This cutoff is chosen as to minimize the misclassification error. For this, we will use our test data. We are interested in the sensitivity ($\frac{TP}{TP+FN}$) and specifity ($\frac{TN}{TN+FP}$) of the final model. We find the following cutoff and corresponding sensitivity and specificity. 

```{r, echo=FALSE}
# lasso on test data
lasso_preds <- predict(mCvLasso, s = mCvLasso$lambda.min, newx = as.matrix(testX), family="binomial",type = "response")

# Inputs:
# * obs: vector of test observations
# * pred: vector of model predictions
# * cutoff_values: vector of prediction thresholds c
calculate_misclass_error <- function(obs, pred, cutoff_values) {
  stopifnot(length(obs) == length(pred))
  misclass_errors <- rep(NA, length(cutoff_values))
  for (i in seq_along(cutoff_values)) {
    cutoff <- cutoff_values[i]
    ypred <- as.numeric(pred > cutoff) # translates TRUE/FALSE to 1/0
    misclass_errors[i] <- mean(ypred != obs) # proportion of misclassifications
  }
  data.frame(
    "cutoff" = cutoff_values,
    "misclass" = misclass_errors
  )
}
result <- calculate_misclass_error(testY, lasso_preds, seq(0.4,0.9,by=0.01))
id <- which.min(result[,2])
cutoff <- result[id, 1]
cutoff
newY <- ifelse(lasso_preds > cutoff, 1, 0)  # everything bigger than cutoff -> predict 1
#newY

# calculate sensitivity and specificity
sens <- sum(newY == 1 & testY==1)/ sum(testY == 1)
spec <- sum(newY == 0 & testY==0)/ sum(testY == 0)
sens 
spec
```


## Conclusions


## References
<div id="refs"></div>

## Appendix: Names of candidate differentiated genes
`r lfdr_genes`
