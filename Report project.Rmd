---
title: "HDDA Project"
author: "Lisa Barbier, Klara Dewitte, Fabienne Haot, Kasimir Putseys"
date: "December 2021"
bibliography: references.bib
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(gridExtra)
library(HDDAData)
library(glmnet)
library(MASS)
library(locfdr)
library(boot)
library(pROC)
library(Rcpp)
library(plotROC)
library(ggplot2)
library(gridExtra)
library(grid)

# graphical parameters
fill_blue = "lightblue"; fill_red = "firebrick"
col_blue = "blue"; col_red = "red"; col_green = "green"
col_response <- c("accept" = col_red, "reject" = col_blue)
fill_response <- c("accept" = fill_red, "reject" = fill_blue)
cex_txt <- 1.5; loc_txt <- 3; adj_txt <- 0.5; padj_txt <- 0
size_txt <- 19
fig_nr <- 1
```

# Executive Summary

We investigated the claim that some genes are responsible for a patientâ€™s likelihood of rejecting a kidney after transplantation. We started with an exploration of the data set by looking at the directions in the data space with the most variability, but this did not help us to make a distinction between the rejected and accepted cases. The variability in the data set does not appear to be related with kidney rejection. Next, we searched again for a linear combination of genes that succeeds in separating the two cases by using linear discriminant analysis. We noticed that we did not need all the genes. A subset of the total number of genes is enough to find a separation. This gives an indication that some genes are indeed associated with kidney rejection, while others are irrelevant. We continued our research by looking at which genes are differentially expressed between the two groups (by comparing means). We found 422 differentiated genes which are given in the appendix. Because of this result, we further assessed if kidney rejection can be predicted from the genes and this seemed to be the case. With only 12 genes, fairly-well predictions can be made (approximately 68% of the rejected kidneys could be predicted from these genes).

# Technical Report


```{r getdata, include = FALSE}
data("Einecke2010Kidney")
X_raw <- Einecke2010Kidney[, -1]
Y <- factor(Einecke2010Kidney[, 1], 
            levels = c(0, 1), labels = c('accept', 'reject'))
```

## Exploration of the Data
The data exploration was started with examining the Einecke 2010 kidney data [@einecke:2010]. Of the 250 observations, there are 67 cases where the kidney transplant was rejected and 183 where it was accepted. Furthermore, no missing values were detected. Next, we take a look at possible outliers. Figure 1 gives the minimum, mean and maximum expression level for every gene. We clearly see that two clouds and the mean are formed for the three values. This figure also illustrates that the data seems to be close to centered, but not scaled, which indicates centering and scaling will be needed for further analysis. Finally, no outliers were found. 

```{r soundness_checks, echo = FALSE, fig.height = 5, fig.width = 10, fig.align = "center"}
# missing values
nr_values  <- sapply(X_raw, function(var) {length(var)})
nr_missing <- sapply(X_raw, function(var) {length(which(is.na(var)))})
nr_unique  <- sapply(X_raw, function(var) {n_distinct(var)})

# outliers
max_per_var <- sapply(Einecke2010Kidney, max)
min_per_var <- sapply(Einecke2010Kidney, min)
mean_per_var <- sapply(Einecke2010Kidney, mean)

tb_summary <- tibble(Name = colnames(Einecke2010Kidney),
                     Nr = seq_along(Einecke2010Kidney),
                     MaxPerVar = max_per_var,
                     MinPerVar = min_per_var,
                     MeanPerVar = mean_per_var)
                     
plot_first_view <- ggplot(data = tb_summary) +
  geom_point(mapping = aes(x = Nr, y = MinPerVar), col = col_blue, size = .3) +
  geom_point(mapping = aes(x = Nr, y = MeanPerVar), col = col_green, size = .3) +
  geom_point(mapping = aes(x = Nr, y = MaxPerVar), col = col_red, size = .3) +
  theme_bw() 
  
title <- paste0("Figure ", fig_nr, ": Data exploration")
grid.arrange(plot_first_view,
             top = textGrob(title, gp = gpar(fontsize = size_txt)))
fig_nr <- fig_nr + 1
X <- scale(X_raw)
```

A first look at the data does not provide a clear understanding on how kidney rejection is associated with the gene expression levels. Considering the data is in a high dimensional setting, we opted for PCA to obtain a better understanding.

```{r PCA_var_explained, echo = FALSE, warning = FALSE, fig.height = 5, fig.width = 10, fig.align = "center"}

# PCA
svd_x <- svd(X)
Z <- svd_x$u %*% diag(svd_x$d) # Calculate the scores
V <- svd_x$v                 # Calculate the loadings
pca_x <- prcomp(X, center = FALSE, scale. = FALSE)

var_explained <- svd_x$d^2 / sum(svd_x$d^2)

dim_var = 1:150
plot_var <- ggplot() +
  geom_point(mapping = aes(x = dim_var, y = var_explained[dim_var]), col = col_red) +
  geom_line(mapping = aes(x = dim_var, y = var_explained[dim_var]), col = col_red) +
  labs(y = "Percent variance explained", x = "PCs") +
  theme_bw()


title <- paste0("Figure ",fig_nr,": Variance explained by the first principal components")
grid.arrange(plot_var,top= textGrob(title, gp=gpar(fontsize = size_txt)))

fig_nr <- fig_nr + 1
```

The first 2 PCs explain 19% of the variability of X and with the first 50 we obtain 63% of the variability of X (Figure `r fig_nr-1`).

```{r PCA_scores, echo = FALSE, warning = FALSE, fig.height = 5, fig.width = 10, fig.align = "center"}

Scores <- X %*% svd_x$v
tb_scores <- tibble(PCA1 = Scores[, 1],
                    PCA2 = Scores[, 2],
                    Object = rownames(Scores),
                    Rejected = as.factor(Y))

p_scatter <- ggplot(tb_scores) +
  geom_point(mapping = aes(x = PCA1, y = PCA2, col = Rejected)) +
  scale_color_manual(values = col_response) +
  labs(title = "A. Scores for 2 PCs") +
  theme_bw()   

p_dens <- ggplot(tb_scores) +
  geom_density(mapping=aes(x = PCA1 + PCA2, fill = Rejected),
               alpha = 0.5) +
  labs(title = "B. Density for 2 PCs") +
  scale_fill_manual(values = fill_response) +
  theme_bw()  

title <- paste0("Figure ", fig_nr, ": Principal component analysis")
grid.arrange(p_scatter, p_dens, ncol = 2,
             top = textGrob(title, gp = gpar(fontsize = size_txt)))
fig_nr <- fig_nr + 1

```

 

To get more insight in the research question, we examine whether kidney rejection is associated with the PCs. For this, we look at Figure `r fig_nr-1`A. 
We notice that the PCs do not clearly separate the accepted and rejected cases. However, we do see that the rejected cases are more present in the bottom left corner and the accepted cases in the top right corner. Figure `r fig_nr-1`B confirms this observation by illustrating that the density curve of the rejected cases (blue) is situated where the values for PC1 and PC2 are low, which corresponds to the bottom left corner of Figure `r fig_nr-1`A. On the other hand, the density curve of the accepted cases (red) is situated where PC1 and PC2 are higher, which corresponds to the top right corner.

Next, it is interesting to get more information on which genes are driving the PCs. A lot of the PCs are very small (near zero) and therefore, we create a histogram of the loadings to get a better visualization of these loadings.

```{r loadings, echo = FALSE, , fig.height = 5, fig.width = 10, fig.align = "center"}

tb_loadings <- tibble(V1 = V[,1],V2= V[,2])
hist_loading1 <- ggplot(tb_loadings)+
  geom_histogram(mapping = aes(x = V1), fill = fill_blue, bins = 50) + 
  geom_vline(mapping = aes(xintercept = quantile(V1, 0.025)), col = col_red) +
  geom_vline(mapping = aes(xintercept = quantile(V1, 0.975)), col = col_red) +
  labs(title = 'A. Histogram PC 1 loadings', x = 'Loading', y = 'Frequency') +
  theme_bw()

  
hist_loading2 <- ggplot(tb_loadings) +
  geom_histogram(mapping = aes(x = V2), fill = fill_blue, bins = 50) + 
  geom_vline(mapping = aes(xintercept = quantile(V2, 0.025)), col = col_red) +
  geom_vline(mapping = aes(xintercept = quantile(V2, 0.975)), col = col_red) +
  labs(title = 'B. Histogram PC 2 loadings', x = 'Loading', y = 'Frequency') +
  theme_bw()

title <- paste0("Figure ", fig_nr, ": Histogram for PC-loadings")
grid.arrange(hist_loading1, hist_loading2, ncol = 2,
             top= textGrob(title, gp = gpar(fontsize = size_txt)))
fig_nr <- fig_nr + 1

```

The histograms (Figures `r fig_nr-1`A and `r fig_nr-1`B) confirm this finding, which gives us a reason to believe that sparse PCA is worthwhile. 

```{r sparse_PCA, echo = FALSE, fig.height = 5, fig.width = 10, fig.align = "center"}

set.seed(45)
fit_loadings1 <- cv.glmnet(X, Z[, 1],
                           alpha = 0.5, nfolds = 5)
set.seed(45)
fit_loadings2 <- cv.glmnet(X, Z[, 2], alpha = 0.5, nfolds = 5)

sparse_loadings1 <- as.vector(coef(fit_loadings1, s = fit_loadings1$lambda.1se))
sparse_loadings2 <- as.vector(coef(fit_loadings2, s = fit_loadings2$lambda.1se))
## How many non-zero loadings do we have (excluding the intercept)?
non_zero1 <- sum(abs(sparse_loadings1[-1]) > 0)
non_zero2 <- sum(abs(sparse_loadings2[-1]) > 0)

SPC1 <- X %*% sparse_loadings1[-1] # without the intercept
SPC2 <- X %*% sparse_loadings2[-1] # without the intercept

tb_sparse_pca <- tibble(SPC1, SPC2, 
                        Rejected = as.factor(Y))
p_scatter_sparse <- ggplot(tb_sparse_pca) +
  geom_point(mapping = aes(x = SPC1, y = SPC2, col = Rejected)) +
  labs(title = paste("A.", non_zero1, "genes for SPC1,", 
                     non_zero2, "for SPC2")) +
  scale_color_manual(values = col_response) +
  theme_bw()  

p_dens_sparse <- ggplot(tb_scores) +
  geom_density(mapping = aes(x = SPC1 + SPC2, fill = Rejected),
               alpha = 0.5) +
  labs(title = "B. Density for sparse PCs") +
  scale_fill_manual(values = fill_response) +
  theme_bw()

title <- paste0("Figure ", fig_nr, ": Visualization of the first two sparse PCs")
grid.arrange(p_scatter_sparse, p_dens_sparse, ncol = 2, 
             top = textGrob(title, gp = gpar(fontsize = size_txt)))
fig_nr = fig_nr + 1
```


For both PC1 and PC2 around 200 genes are important (Figure `r fig_nr-1`), which can provide more or less the same result as the PCA with all the genes (Figure `r fig_nr-3`). This subset of genes are the drivers of the majority of the variability in X. However, because PCA does not separate the rejected and accepted cases well, these genes will not provide much information on whether or not the transplantation will be successful or not. Therefore, it would not be worthwhile to further examine these genes.

Finally, because PCA only takes the genes into account in the decomposition and not the class membership, we executed LDA. Furthermore, because the genes are collinear, a sparse LDA is performed as well. The results of these are illustrated in Figures `r fig_nr`A and `r fig_nr`B.

```{r LDA, warning = FALSE, echo = FALSE, fig.height = 5, fig.width = 10, fig.align = "center"}
kidney.lda <- MASS::lda(x = X, grouping = Y)
Vlda <- kidney.lda$scaling
colnames(Vlda) <- paste0("V", 1:ncol(Vlda))
Zlda <- X%*%Vlda
colnames(Zlda) <- paste0("Z", 1:ncol(Zlda))

#set.seed(45)
lda_loadings <- cv.glmnet(X, Zlda, alpha = 0.5, nfolds = 5)

sparse_lda_loadings <- as.vector(
  coef(lda_loadings, s = lda_loadings$lambda.1se)
)

SLDA <- X %*% sparse_lda_loadings[-1]

# number of non-zero loadings
n_nonzero <- sum(sparse_lda_loadings != 0)
# boxplot
lda_box <- ggplot() +
  geom_boxplot(mapping = aes(y = Zlda, groups = Y, fill = Y)) +
  scale_fill_manual(values = fill_response) +
  labs(title = 'A. LDA with total set of genes', y = 'Rejection status') +
  theme_bw()

slda_box <- ggplot() +
  geom_boxplot(mapping = aes(y = SLDA, groups = Y, fill = Y)) +
  scale_fill_manual(values = fill_response) +
  labs(title = paste0("B. LDA with subset of ",n_nonzero, " genes"), 
       y = 'Rejection status') +
  theme_bw()

title <- paste0("Figure ", fig_nr,": Linear discriminant analysis")
grid.arrange(lda_box, slda_box, ncol = 2,              
             top = textGrob(title, gp = gpar(fontsize = size_txt)))
fig_nr = fig_nr + 1

```

Both the regular LDA (Figure `r fig_nr-1`A) and the sparse LDA (Figure `r fig_nr-1`B) separate the accepted cases (red) and the rejected cases (blue) fairly well. However, the sparse LDA only needs 52 genes to do this. With the sparse LDA we see that the accepted cases are situated just below 0 and the rejected cases around 1.

## Hypothesis Testing

Some genes in the data set are an indicator for kidney rejection. In this section we look for genes which behave differently between the two groups. Since we do not know whether differentiated genes will show higher or lower intensities we opt for 2 sided t-tests with unequal variances. 
\[H_0: \mu_{normal}(gene) = \mu_{reject}(gene) \]
\[H_1: \mu_{normal}(gene) \neq \mu_{reject}(gene) \]

Because we are performing multiple testing we will adjust our p-values, the FDR will be controlled on 10%.

```{r hyptest_tttests, echo = FALSE}
gene_data <- as.matrix(X_raw)
group <- Y
# non adjusted p-values 
ttest_results <- apply(gene_data, 2, function(x){
  t_test <- t.test(x ~ group)
  tibble(stat = t_test$statistic, 
         p_val = t_test$p.value, 
         df = t_test$parameter)})

t_stats <- bind_rows(ttest_results) %>%
  mutate(gene = colnames(X_raw),
         z_val = case_when(stat < 0 ~ qnorm(p_val/2), 
                            TRUE~ qnorm(1-p_val/2)))
```

```{r hyptest_unadjusted, echo = FALSE, fig.height = 5, fig.width = 10, fig.align = "center"}
# selection
nonadj_level <- 0.05
selected_nonadj <- t_stats %>% filter(p_val< nonadj_level)

# plot histogram for the unadjusted z-values
mean_z <- round(mean(t_stats$z_val), 3) 
sd_z <- round(sd(t_stats$z_val), 3) 
threshold_nonadj <- selected_nonadj %>% 
  summarize(left = max(case_when(z_val > mean_z ~ -Inf, TRUE~ z_val)),
            right = min(case_when(z_val < mean_z ~ +Inf, TRUE~ z_val)))

hist_zval <- ggplot(data = t_stats) + 
  geom_histogram(
    mapping = aes(x = z_val, after_stat(density)),
    bins = 20, fill = fill_blue) +
  geom_vline(mapping = aes(xintercept = threshold_nonadj$left),
             color = col_red) +    
  geom_vline(mapping = aes(xintercept = threshold_nonadj$right), color = col_red) +
  geom_line(mapping = aes(x = z_val,y = dnorm(z_val)), color = col_blue) +
  labs(title = "A. Histogram of  z-values",
       x = "z-values",
       y = "density") +
  annotate(geom = "text", x = -7, y = 0.38, 
           label = paste0("mean: ", mean_z, "\nstdev:", sd_z)) +
  theme_bw()

# signal histogram - current behaviour of p-values: uniform?
hist_pval <- ggplot(data = t_stats) +
  geom_histogram(mapping = aes(x = p_val),
                 fill = fill_red, bins = 20) +
  labs(title = 'B. Histogram of p-values', x = "p-value") +
  theme_bw()

title <- paste0("Figure ", fig_nr,": Multiple t-tests unadjusted")
grid.arrange(hist_zval, hist_pval, ncol = 2,          
             top = textGrob(title, gp = gpar(fontsize = size_txt)))
fig_nr = fig_nr + 1


# number of differentiated genes
table_p_nonadj <- table(t_stats$p_val < nonadj_level)
nr_detected_nonadj <- as.numeric(table_p_nonadj['TRUE'])
```

The standard normal curve is too narrow to fit the distribution (Figure `r fig_nr-1`A). This can happen when the genes are highly correlated or when the null hypothesis does not hold. The histogram of the p-values (Figure `r fig_nr-1`B) shows a clear peak for p-values lower than 5%, but there are too many p-values between 5% and 35%. We cannot execute the @BH:1995 procedure now, because this uses the standard normal null distribution to calculate the FDR.

```{r hyptest_fdr_padj, echo = FALSE, fig.height = 5, fig.width = 10, fig.align = "center"}
# adjusted p values
t_stats <- t_stats %>%
  mutate(z_scale = (z_val - mean(z_val))/sd(z_val),
         p_scale = pnorm(z_scale),
         p_adj = p.adjust(p_scale, method = "fdr"))

# these are the detected genes and a histogram of their t-values
fdr_level <- 0.1
selected_fdr <- t_stats %>% filter(p_adj < fdr_level)

# plot histogram
threshold_fdr <- selected_fdr %>% 
  summarize(left = max(case_when(z_scale > 0~-Inf, TRUE~ z_scale)),
            right = min(case_when(z_scale < 0~+Inf, TRUE~ z_scale)))

hist_fdr <- ggplot(t_stats) + 
  geom_histogram(mapping = aes(x = z_scale, after_stat(density)),
                             fill = fill_blue, bins = 20) +
  geom_vline(mapping = aes(xintercept = threshold_fdr$left),
             color = col_red) +    
  geom_line(mapping = aes(x = z_scale, y = dnorm(z_scale)),
            color = col_blue) +
  labs(title = 'A. Histogram of z-values') +
  geom_histogram(mapping = aes(x = z_scale, after_stat(density)),
                             fill = fill_blue, bins = 20) +
  geom_vline(mapping = aes(xintercept = threshold_fdr$left), color = col_red) +    
  geom_line(mapping = aes(x = z_scale, y = dnorm(z_scale)), color = col_blue) +
  theme_bw()            

# signal histogram - current behaviour of p-values: uniform?
hist_pscale <- ggplot(data = t_stats) +
  geom_histogram(mapping = aes(x = p_scale),
                 fill = fill_red, bins = 20, alpha = 0.7) +
  labs(title = 'B. Histogram of p-values') +
  theme_bw()

# monotonous transformation:
pp_adj <- t_stats %>%
  ggplot(aes(x = p_scale,y = p_adj)) +
  geom_point(size = .3, color = col_blue) +
  geom_segment(x = 0, y = 0, xend = 1, yend = 1) +
  geom_hline(yintercept = fdr_level, color = col_red) +
  labs(title = "C. Adjusting the p-values",
       y = "adjusted p-value") +
  theme_bw()

# show histogram z-values +  monotonous adjustment for the p-values
title <- paste0("Figure ", fig_nr,": Multiple t-tests: scaling and FDR-adjustment")
grid.arrange(hist_fdr, hist_pscale, ncol = 2,
             top = textGrob(title, gp = gpar(fontsize = size_txt)))
fig_nr <- fig_nr + 1

# number of detected genes
table_p_adj <- table(t_stats$p_adj < fdr_level)
nr_detected_fdr <- as.numeric(table_p_adj['TRUE'])
```

Standardizing the z-values does not help (Figure `r fig_nr-1`A), the theoretical distribution is now too wide to apply the @BH:1995 procedure.  
We  will use the local approach instead, where we assume that the observed histogram represents a mixture of different Gaussian distributions (@Efron:2007). The parameters of the Gaussian distributions will be estimated with maximum likelihood.  


```{r local_fdr, echo = FALSE, fig.height = 5, fig.width = 10, fig.align = "center"}
#local fdr
par(mfrow=c(1, 1), omi = c(0, 0, 2, 0), oma = c(0, 0, 2, 0))
fdr_x <- locfdr(t_stats$z_val, plot = 4, main = "histogram")
mtext(paste0("Figure ", fig_nr, ": Local false discovery rate"),
      outer = TRUE, adj = adj_txt, side = loc_txt, cex = cex_txt, padj = padj_txt)
fig_nr <- fig_nr + 1

prop_h0 <- fdr_x$fp0['mlest', 'p0']

#control FDR for 10%
FDR_left <- fdr_x$mat[, 'Fdrleft']
z_mat <- fdr_x$mat[, 'x']
lfdr_mat <- fdr_x$mat[, 'fdr']
dens1_mat <- fdr_x$mat[, 'p1f1'] 

id <- which.max(FDR_left[FDR_left < fdr_level])
t_int <- (fdr_level - FDR_left[id])/(FDR_left[id+1] - FDR_left[id])
threshold <- z_mat[id] * (1-t_int) + z_mat[id] * t_int 
lfdr_level <- lfdr_mat[id] * (1-t_int) + lfdr_mat[id] * t_int
cdf_true_positive = sum(dens1_mat[1:id]) + dens1_mat[id+1] * t_int
prop_tp_r <- cdf_true_positive/sum(dens1_mat)
```

The left plot shows the density of both populations (Figure `r fig_nr-1``). The proportion of differentiated genes is expected to be `r round(100*(1 - prop_h0),2)`% (`r round(10000*(1 - prop_h0),0)` genes) for our data set.  The second plot shows the local false discovery rate. Since we want to control the FDR we are interested in the red dashed line. The threshold is the value on the x-axis corresponding with height 10%. This threshold is `r round(threshold, 2)`.  From the third graph we can deduce that for an FDR of 10% or a local fdr-level of `r round(100* lfdr_level,1)`%, we expect `r round(100*prop_tp_r)`% of all differentiated genes to be correctly identified.   

```{r lfdr_genes, echo = FALSE, fig.height = 5, fig.align = "center"}
t_stats <- t_stats %>%
  mutate(
    lfdr = fdr_x$fdr,
    zfdr = (lfdr < lfdr_level) * z_val,
    isdetected_lfdr = factor(lfdr < lfdr_level,
                        levels= c(TRUE, FALSE),
                        labels = c('detected', 'H0 not rejected')))

summ_lfdr <- t_stats %>%
  filter(lfdr < lfdr_level) %>%
  summarize(nr_of_genes = n(), mean_lfdr = mean(lfdr))

data_lfdr <- as_tibble(fdr_x$mat)
plot_lfdr <- ggplot(data_lfdr) + 
  geom_col(mapping = aes(x = x, y = counts/sum(counts)),
           fill = fill_blue) +
  geom_vline(xintercept = threshold, col = col_red) +
  geom_line(mapping = aes(x = x, y = f0/sum(f0)),
            col = col_blue) +
  labs(x = 'z-value', y = 'density') +
  theme_bw()       

title <- paste0("Figure ", fig_nr,": Multiple t-tests: Local FDR")
grid.arrange(plot_lfdr, 
             top = textGrob(title, gp = gpar(fontsize = size_txt)))
fig_nr = fig_nr + 1
lfdr_genes <- t_stats %>% filter(lfdr < lfdr_level)%>% .$gene
```

We have found `r summ_lfdr$nr_of_genes` candidates for differentiated genes. We expect to have about `r round(fdr_level* summ_lfdr$nr_of_genes)` falsely detected genes. The genes detected by the local fdr will be added as an appendix.  

## Model Selection

In the next step we want to predict the rejection status using the gene expression levels. Therefore, we first start with splitting the dataset into a training (70%) and test (30%) dataset. The training set will be used to train the model and tune the hyperparameters (for example the penalty parameter in lasso regression), while the test set will be used to evaluate the out-of-sample performance of our final model. We will center and scale the training set. Afterwards, the same scale and translation factor are applied on the test set.

```{r training_and_test_data, include = FALSE}
set.seed(1234)
trainset <- sample(nrow(X_raw), 0.7*nrow(X_raw))
trainX <- X_raw[trainset, ]
trainX <- scale(trainX, center = TRUE, scale = TRUE)
scale_factor <- attr(trainX, "scaled:scale")
scale_translation <- attr(trainX, "scaled:center")
trainX <- as.data.frame(trainX)

dim(trainX) #175*10000
trainY <- Y[trainset]
testX <- X_raw[-trainset, ]
dim(testX) #75*10000
testY <- Y[-trainset]

for (varnr in seq_along(testX)){
    xbar <- scale_translation[varnr]
    xsd <- scale_factor[varnr]
    testX[[varnr]] <- (testX[[varnr]]-xbar )/xsd}
    
list(tr_x = trainX, tr_y = trainY, test_x = testX, test_y = testY)

train_data <- data.frame(trainY, trainX)
test_data <- data.frame(testY, testX)
```

Here, we will evaluate three prediction models: principal component regression (PCR), ridge regression and lasso regression.

### Principal component regression (PCR)

```{r PCR, include = FALSE}
# Calculate PCA and extract scores
pca_X <- prcomp(trainX)
Z <- pca_X$x

## Total number of available PCs
n_PC <- ncol(Z)
n_PC
```

First, the PCs are calculated and the total number is selected. In our case, `r n_PC` PCs are available in the training dataset.

```{r fullPCR, include = FALSE, warning = FALSE}
fit_data <- data.frame(trainY, Z)

## Logistic regression with all the PC's
full_model <- suppressMessages(glm(trainY ~ ., data = fit_data, family = "binomial"))

set.seed(42)
full_model_cv <- suppressMessages(cv.glm(
  data = fit_data,  glmfit = full_model,
  cost = auc, K = 9  
))

full_model_cv$delta[1] 
```

First we try to fit a model with all 175 PCs, using cross-validation. Via cross-validation will the training set be divided into 9 approximately equal subsets. We use 9 folds because the data set is relatively small, and this should therefore not take too much time. Here we get an area under the curve (AUC) of `r round(full_model_cv$delta[1],2)`. However, because we use all PCs, we got a warning for overfitting the data.


```{r eachPC, echo = FALSE, warning = FALSE, fig.height = 5, fig.width = 10, fig.align = "center"}
auc_cv_glm <- function(trials, data, folds){
  full_model <- suppressWarnings(glm(Y ~ ., data = data, family = "binomial"))
  n_trials <- length(trials)
  error_auc <- rep(0, n_trials)
  comp <- 1
  for (k_try in trials){
    full_model_cv <- suppressWarnings(suppressMessages(
      cv.glm(
        data = data[1:(k_try+1)],  
        glmfit = full_model,
        cost = pROC::auc, 
        K = folds # note: specify the auc function (from pROC) without`()`!
      )))
    error_auc[comp] = full_model_cv$delta[1] 
    comp <- comp + 1
  }
  df_auc <- tibble(auc = error_auc, nr_of_pc = trials)
}

# model PCR for the first k principal components
pca_x <- prcomp(trainX)
Z <- pca_x$x
fit_data <- data.frame(Y = trainY, Z)

# 9-fold Cross-validation on this one particular model, using AUC
# 2 step-procedure to detect k: big steps (10) first
set.seed(42)
trials <- seq(1, nrow(fit_data), 10)
folds <- 9
cvpcr_run1 <- auc_cv_glm(trials, fit_data, folds)
nr_pc_opt1 <- cvpcr_run1 %>% filter(auc == max(auc)) %>% .$nr_of_pc # largest AUC for this number of PCs
plot_pcr_auc1 <- ggplot(cvpcr_run1) +
  geom_line(mapping = aes(x = nr_of_pc, y = auc), col = col_blue) +
  geom_vline(mapping = aes(xintercept = nr_pc_opt1), col = col_red) +
  labs(title = 'A. Cross validation PCR: steps of 10') +
  theme_bw() 

# zoom in: small steps(1) - same seed
set.seed(42)
trials <- seq(1, 20)
folds <- 9
cvpcr_run2 <- auc_cv_glm(trials, fit_data, folds)
nr_pc_opt2 <- cvpcr_run2 %>% filter(auc == max(auc)) %>% .$nr_of_pc  #largest AUC for this number of PCs
plot_pcr_auc2 <- ggplot(cvpcr_run2) +
  geom_line(mapping = aes(x = nr_of_pc, y = auc), col = col_blue) +
  geom_vline(mapping = aes(xintercept = nr_pc_opt2), col = col_red) +
  labs(title = 'B. Cross validation PCR: steps of 1') +
  theme_bw() 

title <- paste0("Figure ", fig_nr, ": Optimal number of PCs")
grid.arrange(plot_pcr_auc1, plot_pcr_auc2, ncol = 2, 
             top = textGrob(title, gp = gpar(fontsize = size_txt)))     
fig_nr <- fig_nr+1
```

In a second step, we loop and repeat this for each PC. First, we loop over the 175 PCs, in steps of 10, to decide in which region we will zoom in (Figure `r fig_nr -1`A). Here, we get `r nr_pc_opt1` as optimal number of PCs. To get a more accurate look, we zoom in between 1 and 20 PCs and repeat the same process, but with steps of 1 (Figure `r fig_nr-1`B). This gives us an optimal number of `r nr_pc_opt2` PCs.

```{r PCRfinal, echo = FALSE, warning = FALSE, fig.align = "center"}
pca <- prcomp(trainX)
Vk <- pca$rotation[, 1:nr_pc_opt2] # the loadings matrix
Zk <- pca$x[, 1:nr_pc_opt2]
pcr_model1 <- glm(trainY ~ Zk, family = "binomial")
#summary(pcr_model1)

pcr_train = tibble(pcr_score = pcr_model1$fitted.values, 
                   rejection = trainY)
                   
plot_dens_pcr <- ggplot(pcr_train) +
  geom_density(mapping = aes(x = pcr_score, fill=rejection), 
               alpha = 0.5) +
  scale_fill_manual(values = fill_response) +
  theme_bw() 

title <- paste0("Figure ", fig_nr, ": PCR on ", nr_pc_opt2, " PCs - density")
grid.arrange(plot_dens_pcr, 
             top = textGrob(title, gp = gpar(fontsize = size_txt)))
fig_nr <- fig_nr+1
```

In the last step, a PCR model with `r nr_pc_opt2` PCs is fitted. The Akaike's Information Criterion (AIC) for this model is 158.43.
When we plot the two densities of the accepted and rejected kidneys with this model (Figure `r fig_nr`), we clearly see there is no overlap. This means, the model is not acceptable enough. 

### Lasso regression

We are now going to use two penalised (logistic) regression models. First, a Lasso regression that uses the $L_1$-norm is performed. We will again only use the training set to fit the model. Figure `r fig_nr`A clearly shows that when the penalty parameter $\lambda$ increases the estimates are shrunken towards zero. When it hits zero, it remains zero. Hence, choosing $\lambda$ is related to feature selection.


```{r Lasso, echo = FALSE, fig.height = 5, fig.width = 10, fig.align = "center"}
par(mfrow=c(1,2), oma = c(0, 0, 5, 0))

# Lasso
mLasso <- glmnet(x = trainX, y = trainY, alpha = 1, family  = "binomial")  # alpha = 1 -> Lasso

plot(mLasso, xvar = "lambda")
#title("A. Coefficients in lasso regression", line = 2)

# cross validation to find optimal lambda
set.seed(44)
K <- 9 # number of folds
mCvLasso <- cv.glmnet(x = as.matrix(trainX), y = as.matrix(trainY), 
                      alpha = 1, family  = "binomial", 
                      type.measure = "auc", nfolds = K)
plot(mCvLasso)
title <- paste0("Figure ", fig_nr,": Lasso regression")
mtext(title, outer = TRUE, cex = cex_txt, line = 3)
mtext("A. Coefficients in lasso regression", adj = 0.1, outer = TRUE, cex = 1, line = 0)
mtext("B. Choosing optimal lambda", adj = 0.7, outer = TRUE, cex = 1, line = 0)
fig_nr <- fig_nr +1
```

To make a decision about $\lambda$, cross-validation is used on the training data set. We use again 9-fold cross-validation as in PCR. We optimize the area under the receiver characteristic curve (AUC).

Two vertical lines are added on figure `r fig_nr`B. The first one gives the $\log(\lambda)$ for which we find the highest AUC. We will go on with this value, because with the other choice for $\lambda$, almost no genes are used anymore. We are left with 12 predictors from the original 10,000 genes (i.e. a sparse solution). The values of the corresponding $\beta$ coefficients are shown with the corresponding names of the genes on figure `r fig_nr`s. These genes are hence the most important in predicting the kidney transplant rejection.

```{r Lasso_optimal, echo = FALSE, out.width = "50%", fig.align = "center"}
# Lasso with optimal lambda
lambda_cv <- mCvLasso$lambda.min # optimal lambda
id_lambda_cv <- which(mCvLasso$lambda == lambda_cv)
auc_lasso <- mCvLasso$cvm[id_lambda_cv]

# Optimal model 
mLassoOpt <- glmnet(x = as.matrix(trainX), y = as.matrix(trainY), 
                    alpha = 1, family  = "binomial", lambda = lambda_cv)
#summary(coef(mLassoOpt))

# with optimal lambda (largest auc) the output shows the non-zero estimated regression coefficients

summ_mLassoOpt <- summary(coef(mLassoOpt))
tb_genes_mLassoOpt <- as_tibble(summ_mLassoOpt) %>%
  filter(i > 1)%>%
  mutate(namegene = colnames(X_raw)[i], 
         gene = i, 
         beta_hat = x)

lasso_genes <- ggplot(tb_genes_mLassoOpt) +
  geom_text(mapping = aes(x = gene, y = beta_hat, label = namegene)) +
  geom_hline(yintercept = 0, color = col_red) +
  theme_bw()

title <- paste0("Figure ", fig_nr, ": Estimated coefficients Lasso")
grid.arrange(lasso_genes, 
             top = textGrob(title, gp = gpar(fontsize = size_txt)))
fig_nr <- fig_nr + 1
```

We have now our optimal Lasso model with $\lambda =$ `r round(lambda_cv,3)` and AUC `r round(auc_lasso,2)` on the training data set, which is relatively high.

### Ridge regression


```{r ridge, echo = FALSE, out.width = "50%",fig.align = "center"}
par(oma = c(0, 0, 3, 0))

# Ridge regression
mRidge <- glmnet(x = as.matrix(trainX), y = as.matrix(trainY), alpha = 0, family  = "binomial")
title <- paste0("Figure ", fig_nr, ": Coefficients ridge regression")
plot(mRidge, xvar = "lambda")
mtext(title, outer = TRUE, cex = cex_txt, line = 1)
fig_nr <- fig_nr +1

# Ridge regression with cross-validation
set.seed(44)
mCvRidge <- cv.glmnet(x = as.matrix(trainX), y = as.matrix(trainY), alpha = 0, family  = "binomial", type.measure = "auc", nfolds = K) # alpha = 0 -> ridge
#plot(mCvRidge)

# Ridge regression with optimal lambda
rlambda_cv <- mCvRidge$lambda.min # optimal lambda
id_rlambda_cv <- which(mCvRidge$lambda == rlambda_cv)
auc_ridge <- mCvRidge$cvm[id_rlambda_cv]
mRidgeOpt <- glmnet(x = as.matrix(trainX), y = as.matrix(trainY), alpha = 0, family  = "binomial", lambda = rlambda_cv)
#summary(coef(mRidgeOpt))
```

The last model that we fit on the training data is the ridge regression model that uses the $L_2$-norm. The major difference between lasso and ridge regression is that ridge regression does not perform feature selection. The model is also fitted with a 10 folds cross validation. The optimal $\lambda$ is significantly higher compared to the Lasso regression model (`r round(rlambda_cv,3)`). The AUC value on the training data set equals `r round(auc_ridge, 2)` which is lower than for the lasso regression model.

## Model evaluation

We decide to continue our analysis with the optimal Lasso regression model because it has the highest AUC value of the 3 implemented models and because it is easier to interpret as it does feature selection.
Next, we evaluate this final model on our test set. First, the corresponding ROC curve is examined (Figure `r fig_nr`). Our model clearly performs better than random allocation of cases to the 2 groups.

```{r rocLasso, echo = FALSE,  out.width = "50%", warning = FALSE, fig.align = "center"}
# plot roc curve for Lasso
dfLassoOpt <- data.frame(
  pred = predict(mCvLasso, s = lambda_cv, newx = as.matrix(testX), family = "binomial", type = "response") %>% c(.),
  known.truth = testY)

roc <-
  dfLassoOpt  %>%
  ggplot(aes(d = known.truth, m = pred)) +
  geom_roc(n.cuts = 0, col = col_blue) +
  xlab("1-specificity (FPR)") +
  ylab("sensitivity (TPR)") +
  theme_bw()

title = paste0("Figure ",fig_nr,": ROC curve")  
grid.arrange(roc, 
             top = textGrob(title, gp = gpar(fontsize = size_txt)))
fig_nr <- fig_nr + 1

```

```{r misclassification op training data, echo = FALSE}
# predictions on training set
lasso_preds_train <- predict(mCvLasso, s = mCvLasso$lambda.min, newx = as.matrix(trainX), family = "binomial", type = "response")

# Inputs:
# * obs: vector of test observations
# * pred: vector of model predictions
# * cutoff_values: vector of prediction thresholds c
calculate_misclass_error <- function(obs, pred, cutoff_values) {
  stopifnot(length(obs) == length(pred))
  misclass_errors <- rep(NA, length(cutoff_values))
  for (i in seq_along(cutoff_values)) {
    cutoff <- cutoff_values[i]
    ypred <- as.numeric(pred > cutoff) # translates TRUE/FALSE to 1/0
    misclass_errors[i] <- mean(ypred != obs) # proportion of misclassifications
  }
  data.frame(
    "cutoff" = cutoff_values,
    "misclass" = misclass_errors
  )
}
# make sure we have the right levels in the training en test set
levels(trainY)[levels(trainY) == "accept"] <- 0
levels(trainY)[levels(trainY) == "reject"] <- 1
levels(testY)[levels(testY) == "accept"] <- 0
levels(testY)[levels(testY) == "reject"] <- 1

# optimize cutoff on the training set
result <- calculate_misclass_error(trainY, lasso_preds_train, seq(0.1, 0.9, by = 0.01))
id <- which.min(result[, 2])
cutoff <- result[id, 1]  # cutoff optimized on training data

# how does it perform on the test set with the chosen cutoff
lasso_preds_test <- predict(mCvLasso, s = mCvLasso$lambda.min, newx = as.matrix(testX), family = "binomial",type = "response")
newY <- ifelse(lasso_preds_test > cutoff, 1, 0)  # everything bigger than cutoff -> predict 1

# calculate sensitivity and specificity
sens <- sum(newY == 1 & testY==1)/ sum(testY == 1)
spec <- sum(newY == 0 & testY==0)/ sum(testY == 0)
```

A high AUC value is also found on the test set (`r round(calc_auc(roc)[1,3],2)`). The next step is to determine a good threshold $c$ as a prediction cut-off. This cutoff is selected in order to minimize the misclassification error. For this, we will use our training data again. Once the cutoff is determined, we assess the performance of our final model on a test set that contains observations we did not use to fit the model. The goal is to understand how well it predicts the right group for new observations. For this, we are interested in the sensitivity ($\frac{TP}{TP+FN}$) and specificity ($\frac{TN}{TN+FP}$) of the final model. With the optimized cutoff `r cutoff`, we obtain sensitivity `r round(sens,2)` and specificity `r round(spec,2)`. Both measures are satisfactory. The specificity is higher than the sensitivity which means the model does a better job in predicting the accepted cases than the rejected ones.


## References
<div id="refs"></div>

## Appendix: Names of candidate differentiated genes
`r lfdr_genes`
