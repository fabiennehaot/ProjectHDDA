---
title: "HDDA Project"
author: "Lisa Barbier, Klara Dewitte, Fabienne Haot, Kasimir Putseys"
date: "December 2021"
bibliography: references.bib
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(gridExtra)
library(HDDAData)
library(glmnet)
library(MASS)
library(locfdr)
library(boot)
library(pROC)
library(Rcpp)
```

# Research Question

# Executive Summary

# Technical Report

## Data

Data description in [@einecke:2010]
```{r getdata}
data("Einecke2010Kidney")
X_raw <- Einecke2010Kidney[, -1]
Y <- factor(Einecke2010Kidney[, 1], 
            levels = c(0,1),labels = c('accept', 'reject'))
```

## Exploration of the Data
The data exploration is started by examining the Einecke 2010 kidney data. Of the 250 observations, there are 67 cases where the kidney transplant was rejected and 183 where it was accepted. Furthermore, no missing values were detected. Next, we look at possible outliers. The following plot illustrates that the data seem to be close to centered but not scaled. Finally, no outliers were found. The following plot illustrates this by the minimum, mean and maximum expression level for every gene. We clearly see that three clouds are formed for the three values.

```{r soundness checks, echo=FALSE }
#missing values
nr_values  <- sapply(X_raw, function(var) {length(var)})
nr_missing <- sapply(X_raw, function(var) {length(which(is.na(var)))})
nr_unique  <- sapply(X_raw, function(var) {n_distinct(var)})

#outliers
max_per_var <- sapply(Einecke2010Kidney, max)
min_per_var <- sapply(Einecke2010Kidney, min)
mean_per_var <- sapply(Einecke2010Kidney, mean)
tb_summary <- tibble(Name= colnames(Einecke2010Kidney),
                     Nr = seq_along(Einecke2010Kidney),
                     MaxPerVar = max_per_var,
                     MinPerVar = min_per_var,
                     MeanPerVar = mean_per_var)
ggplot(data=tb_summary) +
  geom_point(mapping = aes(x = Nr, y = MinPerVar), col = 'blue', size = .3)+
  geom_point(mapping = aes(x = Nr, y = MeanPerVar), col = 'green', size = .3) +
  geom_point(mapping = aes(x = Nr, y = MaxPerVar), col = 'red', size = .3) +
  ggtitle("Figure 1")
  theme_bw()

X <- scale(X_raw)

```

A first look at the data does not provided a clear understanding on how kidney rejection is associated with the gene expression levels. Because the data is in a high dimensional setting, we opted for PCA to obtain a better understanding.


```{r PCA, echo=FALSE}
#PCA
svd_x <- svd(X)
Z <- svd_x$u %*% diag(svd_x$d) # Calculate the scores
V <- svd_x$v                 # Calculate the loadings
pca_x <- prcomp(X, center = FALSE, scale. = FALSE)

par(pch = 19, mfrow = c(1, 2))
var_explained <- svd_x$d^2 / sum(svd_x$d^2)
plot(var_explained,
     type = "b", ylab = "Percent variance explained", xlab = "PCs",
     title = "Figure 2: ",
     col = 2
)

Scores <- X %*% svd_x$v
tb_scores <- tibble(PCA1 = Scores[,1],
                    PCA2 = Scores[,2],
                    Object = rownames(Scores),
                    Rejected = as.factor(Y))

ggplot(tb_scores) +
  geom_point(mapping = aes(x = PCA1, y = PCA2, col = Rejected)) +
  labs(title = "Figure 3: Scores for 2 PCA's") +
  theme_bw()  

<<<<<<< HEAD
p_dens <- ggplot(tb_scores)+
  geom_density(mapping=aes(x= PCA1+PCA2, fill=Rejected),
               alpha = 0.5)+
  labs(title="Density for 2 PC's: densities")+
  scale_fill_manual(values=c('firebrick','lightblue'))+
  theme_bw()  

grid.arrange(p_scatter,p_dens,ncol=2)

=======
>>>>>>> 8a5870e9576e358d3b06e9ff855fc12feb7c755c
```

The first 2 PC’s explain 19% of the variability of X and with the first 50 PC’s we obtain 63% of the variability of PC’s. 

To get more insight in the research question, we examine whether kidney rejection is associated the first two PC’s, the following plot illustrates this. 

We see the PC’s do not clearly separate the accepted and rejected cases. However, we do see that the rejected cases are more present in the bottom left corner and the accepted cases in top right corner.

Next, it is interesting to get more information on which genes are driving the PC’s. A lot of the PC’s are very small (near zero) and therefore, we create a histogram of the loadings to get a better visualization of these loadings.

```{r loadings, echo=FALSE}
par(pch = 19, mfrow = c(1, 2))
hist(V[, 1], breaks = 50, xlab = "PC 1 loadings", main = "")
abline(v = c(
  quantile(V[, 1], 0.05),
  quantile(V[, 1], 0.95)
), col = "red", lwd = 2)

hist(V[, 2], breaks = 50, xlab = "PC 2 loadings", main = "")
abline(v = c(
  quantile(V[, 2], 0.05),
  quantile(V[, 2], 0.95)
), col = "red", lwd = 2)

```

The histograms confirm this which gives us a reason to believe that sparse PCA is worthwhile. 

```{r sparce PCA, echo=FALSE}
par(pch = 19, mfrow = c(1, 2))
set.seed(45)
fit_loadings1 <- cv.glmnet(X, Z[, 1],
                           alpha = 0.5, nfolds = 5)
set.seed(45)
fit_loadings2 <- cv.glmnet(X, Z[, 2], alpha = 0.5, nfolds = 5)

sparse_loadings1 <- as.vector(coef(fit_loadings1, s = fit_loadings1$lambda.1se))
sparse_loadings2 <- as.vector(coef(fit_loadings2, s = fit_loadings2$lambda.1se))
## How many non-zero loadings do we have (excluding the intercept)?
non_zero1 <- sum(abs(sparse_loadings1[-1]) > 0)
non_zero2 <- sum(abs(sparse_loadings2[-1]) > 0)

SPC1 <- X %*% sparse_loadings1[-1] # without the intercept
SPC2 <- X %*% sparse_loadings2[-1] # without the intercept

par(mfrow = c(1, 2))
cols <- c("0" = "red", "1" = "blue")
plot(Z[, 1], Z[, 2],
     col = cols[Y], xlab = "PC1", ylab = "PC2", pch = 16,
     main = "All genes \nfor PC1 and PC2"
)
plot(SPC1, SPC2,
     col = cols[Y], xlab = "SPC1", ylab = "SPC2", pch = 16,
     main = paste(non_zero1, "genes for SPC1 \n and", non_zero2, "genes for SPC2")
)

```

For both PC1 and PC2 around 200 genes are important, which can provide more or less the same result as the PCA with all the genes.

Finally, because PCA only takes the genes into account in the decomposition and not the class membership, we executed LDA. Furthermore, because the genes are collinear, a sparse LDA is performed as well. The results of these are illustrated in the plots below.

```{r LDA, warning=FALSE, echo=FALSE}
kidney.lda <- MASS::lda(x = X, grouping = Y)
Vlda <- kidney.lda$scaling
colnames(Vlda) <- paste0("V", 1:ncol(Vlda))
Zlda <- X%*%Vlda
colnames(Zlda) <- paste0("Z", 1:ncol(Zlda))
par(mfrow = c(1, 2))
boxplot(Zlda ~ Y, col = cols, ylab = expression("Z"[1]),
        main = "LDA with total set of genes")
set.seed(45)
lda_loadings <- cv.glmnet(X, Zlda, alpha = 0.5, nfolds = 5)


sparse_lda_loadings <- as.vector(
  coef(lda_loadings, s = lda_loadings$lambda.1se)
)

SLDA <- X %*% sparse_lda_loadings[-1]

# number of non-zero loadings
n_nonzero <- sum(sparse_lda_loadings != 0)
boxplot(SLDA ~ Y,
        col = cols, ylab = "SLDA",
        main = sprintf("LDA with subset of %d genes", n_nonzero)
)
```
<<<<<<< HEAD
We see that the accepted cases are situated just below 0 and the rejected cases around 1.
=======

Both the regular LDA (left) and the sparse LDQ separate the accepted cases (red) and the rejected cases (blue) fairly well. However, the sparse LDA only needs 52 genes to do this. With the sparse LDA we see that the accepted cases are situated just below 0 and the rejected cases around 1.
>>>>>>> 8a5870e9576e358d3b06e9ff855fc12feb7c755c

## Hypothesis Testing 
Not every gene in the data set is an indicator for kidney rejection. In this section we find out which genes behave differently between the two kidney rejection groups. Since we do not know a priori whether differentiated genes will show higher or lower intensities we opt for 2 sided t-tests with unequal variances. 

\[H_0: \mu_{normal}(gene) = \mu_{reject}(gene) \]
\[H_1: \mu_{normal}(gene) \neq \mu_{reject}(gene) \]

We first perform these t-tests for every gene in the data, with an individual significance level of 5%. Because we are performing multiple testing we would expect to find about 500 genes with a p-value lower than 5% if the null hypothesis holds for all the genes. 

```{r hyptest_tttests, echo = FALSE}
gene_data <- as.matrix(X_raw)
group <- Y
# non adjusted p-values 
ttest_results <- apply(gene_data, 2, function(x){
  t_test <- t.test(x ~ group)
  tibble(stat = t_test$statistic, 
         p_val = t_test$p.value, 
         df = t_test$parameter)})

t_stats <- bind_rows(ttest_results) %>%
  mutate(gene = colnames(X_raw),
         z_val= case_when(stat < 0 ~ qnorm(p_val/2), 
                            TRUE~ qnorm(1-p_val/2)))
```

```{r hyptest_unadjusted, fig.height= 4, echo = FALSE}
# selection
nonadj_level <- 0.05
selected_nonadj <- t_stats %>% filter(p_val< nonadj_level)

# plot histogram for the unadjusted z-values
mean_z <- round(mean(t_stats$z_val), 3) 
sd_z <- round(sd(t_stats$z_val), 3) 
threshold_nonadj <- selected_nonadj %>% 
  summarize(left = max(case_when(z_val > mean_z ~ -Inf, TRUE~ z_val)),
            right = min(case_when(z_val < mean_z ~ +Inf, TRUE~ z_val)))

hist_zval <- ggplot(data=t_stats) + 
  geom_histogram(
<<<<<<< HEAD
    mapping = aes(x = z_val,after_stat(density)),
    bins = 20, alpha = .7, fill='lightblue') +
  geom_vline(mapping=aes(xintercept = threshold_nonadj$left),
             color = 'red')+    
  geom_vline(mapping=aes(xintercept = threshold_nonadj$right),
             color = 'red')+
  geom_line(mapping=aes(x = z_val,y = dnorm(z_val)),
            color = 'blue')+
  labs(title = "Histogram of z-values", x = "z-value", y= "density")+
  annotate(geom="text", x=-7, y=0.38, 
           label=paste0(" mean: ", mean_z,"\nstdev: ", sd_z))+
=======
    mapping = aes(x = z_val, after_stat(density)),
    bins = 20, alpha = .7, fill = 'lightblue') +
  geom_vline(mapping = aes(xintercept = threshold_nonadj$left), color= 'red')+    
  geom_vline(mapping = aes(xintercept = threshold_nonadj$right), color= 'red')+
  geom_line(mapping = aes(x = z_val,y = dnorm(z_val)), color = 'blue')+
  labs(title = "multiple t-tests unadjusted",
       subtitle="histogram of corresponding z-values",
       x = "z-values",
       y = "probability density")+
  annotate(geom = "text", x = -7, y = 0.38, 
           label = paste0("mean: ", mean_z, "\nstdev:", sd_z))+
>>>>>>> 8a5870e9576e358d3b06e9ff855fc12feb7c755c
  theme_bw()

# signal histogram - current behaviour of p-values: uniform?
hist_pval <- ggplot(data = t_stats) +
  geom_histogram(mapping = aes(x = p_val),
<<<<<<< HEAD
                 fill = "firebrick", bins = 20, alpha=.7) +
  labs(title = 'Histogram of p-values', x = "p-value") +
=======
                 fill = "firebrick", breaks = seq(0,1,.05), alpha = .7) +
  labs(title = 'histogram of p-values') +
>>>>>>> 8a5870e9576e358d3b06e9ff855fc12feb7c755c
  theme_bw()

grid.arrange(hist_zval, hist_pval, ncol=2,
             top = "Multiple t-tests unadjusted")

# number of differentiated genes
table_p_nonadj <- table(t_stats$p_val < nonadj_level)
nr_detected_nonadj <- as.numeric(table_p_nonadj['TRUE'])

```

There are `r nr_detected_nonadj` out of 10000 genes with non-adjusted p-values below 0.05. The selected genes are mostly found in the lower tail, the negative z-values indicate higher intensities in the group with rejected kidneys.  
The distribution is not standard normal as the observations are skewed to the left, the standard deviation is also much larger than 1. This can happen when the genes are highly correlated or when the null hypothesis does not hold.  
The histogram of the p-values is not uniform, a clear peak is visible for p-values lower than 5%, but the frequencies for values between 5% and 35% drop too slowly, again because of the large spread of the distribution.  
We will standardize the z values, recalculate the associated p-values and adjust for multiple testing by controlling the FDR to 10%. The p-values are adjusted using the @BH:1995 procedure.  

```{r hyptest_fdr_padj, echo = FALSE, fig.height=4}
# adjusted p values
t_stats <- t_stats %>%
  mutate(z_scale = (z_val - mean(z_val))/sd(z_val),
         p_scale = pnorm(z_scale),
         p_adj = p.adjust(p_scale, method = "fdr"))

# these are the detected genes and a histogram of their t-values
fdr_level <- 0.1
selected_fdr <- t_stats %>% filter(p_adj < fdr_level)

# plot histogram
threshold_fdr <- selected_fdr %>% 
  summarize(left = max(case_when(z_scale > 0~-Inf, TRUE~ z_scale)),
            right = min(case_when(z_scale < 0~+Inf, TRUE~ z_scale)))

hist_fdr <- ggplot(t_stats) + 
<<<<<<< HEAD
  geom_histogram(mapping=aes(x = z_scale, after_stat(density)),
                             fill="lightblue", bins = 20, alpha=.7)+
  geom_vline(mapping=aes(xintercept=threshold_fdr$left),
             color='red') +    
  geom_line(mapping = aes(x=z_scale, y= dnorm(z_scale)),
            color='blue') +
  labs(title = 'Histogram of z-values') +
=======
  geom_histogram(mapping = aes(x = z_scale, after_stat(density)),
                             fill = "lightblue", bins = 20, alpha = .7) +
  geom_vline(mapping = aes(xintercept = threshold_fdr$left), color = 'red') +    
  geom_line(mapping = aes(x = z_scale, y = dnorm(z_scale)), color = 'blue') +
  labs(title = "multiple t-tests FDR",
       caption = 'histogram of transformed z-values, rescaled and adjusted for multiple testing') +
>>>>>>> 8a5870e9576e358d3b06e9ff855fc12feb7c755c
  theme_bw()            

# signal histogram - current behaviour of p-values: uniform?
hist_pscale <- ggplot(data = t_stats) +
  geom_histogram(mapping = aes(x = p_scale),
<<<<<<< HEAD
                 fill = "firebrick", bins = 20, alpha=.7) +
  labs(title = 'Histogram of p-values') +
=======
                 fill = "firebrick", breaks = seq(0, 1, .05), alpha = .7) +
  labs(title = 'histogram of p-values') +
>>>>>>> 8a5870e9576e358d3b06e9ff855fc12feb7c755c
  theme_bw()

# monotonous transformation:
pp_adj <- t_stats %>%
  ggplot(aes(x = p_scale,y = p_adj)) +
  geom_point(size = .3,color='blue') +
  geom_segment(x = 0, y = 0, xend = 1, yend = 1) +
  geom_hline(yintercept = fdr_level, color = 'red') +
  labs(title = "Adjusting the p-values",
<<<<<<< HEAD
       y= "adjusted p-value") +
  theme_bw()

# show histogram z-values +  monotonous adjustment for the p-values
grid.arrange(hist_fdr, hist_pscale, pp_adj, ncol=3,
             top  = "Multiple t-tests: scaling and FDR-adjustment")
=======
       y = "adjusted p-value (BH, 1995)") +
  theme_bw()

# show histogram z-values +  monotonous adjustment for the p-values
grid.arrange(hist_fdr, hist_pscale, pp_adj, ncol = 3)
>>>>>>> 8a5870e9576e358d3b06e9ff855fc12feb7c755c

# number of detected genes
table_p_adj <- table(t_stats$p_adj < fdr_level)
nr_detected_fdr <- as.numeric(table_p_adj['TRUE'])

```

The scaling has lowered the number of candidate genes drastically. 
There are now `r nr_detected_fdr` genes with adjusted p-value below 10%. All of the detected genes are now found in the lower tail.  
The histogram of the non selected genes is still skewed to the left. Both the density plot and the histogram of the p-values show that the assumption of a normal distribution is violated. The graph to the right shows that the probability of a false detection rises very quickly, the number of expected false discoveries grows fast because the theoretical curve is wider than should be. The @BH:1995 procedure is too conservative for this data set.   
We believe we can detect more differentiated genes using a local approach where we assume that the observed histogram represents a mixture of different Gaussian distributions, each with their own mean and standard deviance (@Efron:2007). We will start from the original z-values, the parameters of the Gaussian distributions will be estimated with maximum likelihood.  


```{r localfdr, echo = FALSE, fig.height=4}
#local fdr
fdr_x <- locfdr(t_stats$z_val, plot = 4) 
prop_h0 <- fdr_x$fp0['mlest','p0']
<<<<<<< HEAD

#control FDR for 10%
FDR_left <- fdr_x$mat[,'Fdrleft']
z_mat <- fdr_x$mat[,'x']
lfdr_mat <- fdr_x$mat[,'fdr']
dens1_mat <- fdr_x$mat[,'p1f1'] 
=======
```

The left plot shows the density of both populations. The proportion of differentiated genes is expected to be `r round(100*(1 - prop_h0),2)`%. This means `r round(10000*(1 - prop_h0),0)` genes for our data set. We do not expect to find signal genes in the right tail. 

The second plot shows the local false discovery rate. Since we want to control the FDR we are interested in the red dashed line. The height of the left FDR-graph is the FDR for all the genes with z-values smaller than the threshold shown on the x-axis.

```{r control_lfdr}
FDR_left <- fdr_x$mat[, 'Fdrleft']
z_mat <- fdr_x$mat[, 'x']
lfdr_mat <- fdr_x$mat[, 'fdr']
dens1_mat <- fdr_x$mat[, 'p1f1'] 
>>>>>>> 8a5870e9576e358d3b06e9ff855fc12feb7c755c

id <- which.max(FDR_left[FDR_left < fdr_level])
t_int <- (fdr_level - FDR_left[id])/(FDR_left[id+1] - FDR_left[id])
threshold <- z_mat[id] * (1-t_int) + z_mat[id]*t_int 
lfdr_level <- lfdr_mat[id] * (1-t_int) + lfdr_mat[id]*t_int
cdf_true_positive = sum(dens1_mat[1:id]) + dens1_mat[id+1]* t_int
prop_tp_r <- cdf_true_positive/sum(dens1_mat)
```

The left plot shows the density of both populations. The proportion of differentiated genes is expected to be `r round(100*(1 - prop_h0),2)`%. This means `r round(10000*(1 - prop_h0),0)` genes for our data set. We do not expect to find signal genes in the right tail.  
The second plot shows the local false discovery rate. Since we want to control the FDR we are interested in the red dashed line. The height of the left FDR-graph is the FDR for all the genes with z-values smaller than the threshold shown on the x-axis.  
We want to control the FDR for 10%, for this we can find the value on the x-axis for which the left FDR - graph is equal to 10% (or calculate the threshold using interpolation), the threshold for the z-values becomes `r round(threshold,3)`.  
The third graph returns the probability that a non-null gene can be detected when the nominal local fdr is set at a given local fdr-level. The local fdr-level associated with an FDR of 10% is `r round(100* lfdr_level,1)`% which means that we expect `r round(100*prop_tp_r)`% of all differentiated genes to be correctly identified.   

<<<<<<< HEAD
```{r lfdr_genes, echo = FALSE, fig.height = 4}

=======
```{r lfdr_genes}
>>>>>>> 8a5870e9576e358d3b06e9ff855fc12feb7c755c
t_stats <- t_stats %>%
  mutate(
    lfdr = fdr_x$fdr,
    zfdr = (lfdr < lfdr_level) * z_val,
    isdetected_lfdr = factor(lfdr < lfdr_level,
                        levels= c(TRUE,FALSE),
                        labels = c('detected','H0 not rejected')))
summ_lfdr <- t_stats %>%
  filter(lfdr < lfdr_level) %>%
  summarize(nr_of_genes = n(), mean_lfdr = mean(lfdr))

<<<<<<< HEAD
data_lfdr <- as_tibble(fdr_x$mat)
plot_lfdr <- ggplot(data_lfdr) + 
  geom_col(mapping=aes(x = x, y=counts/sum(counts)),
                 fill="lightblue")+
  geom_vline(xintercept = threshold, col='red')+
  geom_line(mapping= aes(x=x,y=f0/sum(f0)),
            col='blue')+
  labs(x = 'z-value', y= 'density')+
=======
ggplot(t_stats) + 
  scale_fill_manual(values = c("firebrick", "lightblue")) +
  geom_histogram(mapping = aes(x = stat, fill = isdetected_lfdr),
                 bins = 20,alpha = .7) +
  labs(title= 't-statistics for differentiated genes Local FDR') +
>>>>>>> 8a5870e9576e358d3b06e9ff855fc12feb7c755c
  theme_bw()       

grid.arrange(plot_lfdr,top="Multiple t-tests: Local FDR")

lfdr_genes <- t_stats %>% filter(lfdr < lfdr_level)%>% .$gene
```

We have found `r summ_lfdr$nr_of_genes` candidates for differentiated genes. The mean of the local fdr for the detected genes in the data set is `r round(100*summ_lfdr$mean_lfdr,1)`%, we expect to have `r round(summ_lfdr$mean_lfdr* summ_lfdr$nr_of_genes)` to `r round(fdr_level* summ_lfdr$nr_of_genes)` falsely detected genes. The genes detected by the local fdr will be added as an appendix.  

## Model Selection
In the next step we want the predict the rejection status using the gene expression levels. Therefore, we first start with splitting the dataset into a train (70%) and test (30%) dataset. The training set will be used to train the model and tune the hyperparameters (for example the penalty parameter in lasso regression), while the test set will be used to evaluate the out-of-sample performance of our final model.

```{r training_and_test_data, include = FALSE}
set.seed(1234)
trainset <- sample(nrow(X_raw), 0.7*nrow(X_raw))
trainX <- X_raw[trainset, ]
trainX <- scale(trainX, center = TRUE, scale = TRUE)
scale_factor <- attr(trainX,"scaled:scale")
scale_translation <- attr(trainX,"scaled:center")
trainX <- as.data.frame(trainX)

dim(trainX) #175*10000
trainY <- Y[trainset]
testX <- X_raw[-trainset, ]
dim(testX) #75*10000
testY <- Y[-trainset]

for (varnr in seq_along(testX)){
    xbar <- scale_translation[varnr]
    xsd <- scale_factor[varnr]
    testX[[varnr]] <- (testX[[varnr]]-xbar )/xsd}
    
list(tr_x = trainX, tr_y = trainY, test_x = testX, test_y = testY)

train_data <- data.frame(trainY, trainX)
test_data <- data.frame(testY, testX)
```
Here, we will evaluate three prediction models: principal component regression (PCR), Ridge regression and Lasso regression.

# Principal compentent regression (PCR)

```{r PCR, include=FALSE}
# Calculate PCA and extract scores
pca_X <- prcomp(trainX)
Z <- pca_X$x

## Total number of available PCs
n_PC <- ncol(Z)
n_PC
```
First, the principal components are calculated and the total number of available PC's is selected. In our case, `r n_PC` PC's are available.

```{r fullPCR, include = FALSE, warning = FALSE}
fit_data <- data.frame(trainY, Z)

## Logistic regression with all the PC's
full_model <- suppressMessages(glm(trainY ~ ., data = fit_data, family = "binomial"))

set.seed(42)
full_model_cv <- suppressMessages(cv.glm(
  data = fit_data,  glmfit = full_model,
  cost = auc, K = 9  
))

full_model_cv$delta[1] 
```
First we try to fit a model with all the PCs, using crossvalidation. Via crossvalidation will the training set be divived into 9 approximately equal subsets. This means we will have overfitting, as the model will predict the training dataset exactly, but this will not be reproducible to other (or the test) dataset. Here we get an area under the curve (AUC) of `r full_model_cv$delta[1]`.

```{r eachPC, echo = FALSE, warning = FALSE}
auc_cv_glm <- function(trials, data, folds){
  full_model <- suppressWarnings(glm(Y ~ ., data = data, family = "binomial"))
  n_trials <- length(trials)
  error_auc <- rep(0, n_trials)
  comp <- 1
  for (k_try in trials){
    full_model_cv <- suppressWarnings(suppressMessages(
      cv.glm(
        data = data[1:(k_try+1)],  
        glmfit = full_model,
        cost = pROC::auc, 
        K = folds # note: specify the auc function (from pROC) without`()`!
      )))
    error_auc[comp] = full_model_cv$delta[1] 
    comp <- comp + 1
  }
  df_auc <- tibble(auc = error_auc, nr_of_pc = trials)
}

# model PCR for the first k principal components
pca_x <- prcomp(trainX)
Z <- pca_x$x
fit_data <- data.frame(Y = trainY, Z)

# 9-fold Cross-validation on this one particular model, using AUC
# 2 step-procedure to detect k: big steps (10) first
set.seed(42)
trials <- seq(1, nrow(fit_data), 10)
folds <- 9
cvpcr_run1 <- auc_cv_glm(trials, fit_data, folds)
nr_pc_opt1 <- cvpcr_run1 %>% filter(auc == max(auc)) %>% .$nr_of_pc
ggplot(cvpcr_run1) +
  geom_line(mapping = aes(x = nr_of_pc, y = auc), col = 'blue') +
  geom_vline(mapping = aes(xintercept = nr_pc_opt1),col = 'firebrick')+
  labs(title = 'cross validation PCR: steps of 10')+
  theme_bw() -> p1

nr_pc_opt1 # largest AUC for this number of PC's

# zoom in: small steps(1) - same seed
set.seed(42)
trials <- seq(1, 20)
folds <- 9
cvpcr_run2 <- auc_cv_glm(trials, fit_data, folds)
nr_pc_opt2 <- cvpcr_run2 %>% filter(auc == max(auc)) %>% .$nr_of_pc
ggplot(cvpcr_run2) +
  geom_line(mapping = aes(x = nr_of_pc, y = auc), col = 'blue')+
  geom_vline(mapping = aes(xintercept = nr_pc_opt2), col = 'firebrick')+
  labs(title ='cross validation PCR: steps of 1') +
  theme_bw() -> p2

nr_pc_opt2 #largest AUC for this number of PC's

grid.arrange(p1, p2, ncol = 2) # plot the 2 plots next to each other

```
In a second step, we loop and repeat this for each PC. First, we loop over the 175 PC's, in steps of 10, to decide in which region we will zoom in. Here, we get `r nr_pc_opt1` as optimal number of PC's. To get a more accurate look, we zoom in between 1 and 20 PC's and repeat the same process, but wit steps of 1. This gives us an optimal number of PC's of `r nr_pc_opt2`.

```{r PCRfinal, echo = FALSE, warning = FALSE}
pca <- prcomp(trainX)
Vk <- pca$rotation[, 1:nr_pc_opt2] # the loadings matrix
Zk <- pca$x[, 1:nr_pc_opt2]
pcr_model1 <- glm(trainY ~ Zk, family = "binomial")
summary(pcr_model1)

pcr_train = tibble(pcr_score = pcr_model1$fitted.values, 
                   rejection = trainY)
<<<<<<< HEAD
p_dens_pcr <- ggplot(pcr_train)+
  geom_density(mapping=aes(x= pcr_score, fill=rejection), 
               alpha = 0.5)+
  labs(title="Density for 2 PC's: densities")+
  scale_fill_manual(values=c('firebrick','lightblue'))+
=======

p_dens_pcr <- ggplot(pcr_train) +
  geom_density(mapping = aes(x = pcr_score, fill = rejection), alpha = 0.5)+
  labs(title = "Density for 2 PC's: densities") +
  scale_fill_manual(values = c('red','blue')) +
>>>>>>> 8a5870e9576e358d3b06e9ff855fc12feb7c755c
  theme_bw() 

p_dens_pcr
```
In the last step, a principle component regression model with `r nr_pc_opt2` is fitted. The Akaike's Information Criterion (AIC) for this model is 158.43.
When we plot the two densities of the accepted and rejected kidneys with this model, we clearly see there is no overlap. This means, the model is not acceptable enough. 

# Lasso regression

We are now going to use two penalised (logistic) regression models. We start with Lasso regression that uses the $L_1$-norm. We will again only use the training set to fit the model. The following figure nicely shows that when the penality parameter $\lambda$ increases the estimates are shrunken towards zero. When it hits zero, it remains zero. Hence, choosing $\lambda$ is related to feature selection.


```{r, echo=FALSE,fig.show="hold", out.width="50%"}
# Lasso
mLasso <- glmnet(x = trainX, y = trainY, alpha = 1, family  = "binomial")  # alpha = 1 -> Lasso
plot(mLasso, xvar = "lambda")

# cross validation to find optimal lambda
set.seed(44)
K <- 10 #number of folds
mCvLasso <- cv.glmnet(x = as.matrix(trainX), y = as.matrix(trainY), alpha = 1, family  = "binomial", type.measure = "auc", nfolds = K)
plot(mCvLasso)
```
To make a decision about $\lambda$, we will use cross-validation on the training data set. We use 10-fold cross-validation because our dataset is quite small, so it should not take too much time. We optimize the area under the receiver characteristic curve (AUC).

Two vertical lines are added on the figure. The first one gives the $\log(\lambda)$ for which we find the highest AUC. We will go on with this value, because with the other choice for $\lambda$, almost no genes are used anymore. We are left with 12 predictors from the original 10,000 genes (sparse solution). The values of the corresponding $\beta$ coefficients are shown on the following plot.

```{r, echo=FALSE,out.width="50%"}
# Lasso with optimal lambda
lambda_cv <- mCvLasso$lambda.min # optimal lambda
id_lambda_cv <- which(mCvLasso$lambda == lambda_cv)
auc_lasso <- mCvLasso$cvm[id_lambda_cv]

# Optimal model 
mLassoOpt <- glmnet(x = as.matrix(trainX), y = as.matrix(trainY), alpha = 1, family = "binomial", lambda = lambda_cv)
#summary(coef(mLassoOpt))

# with optimal lambda (largest auc) the output shows the non-zero estimated regression coefficients
library(ggplot2)
qplot(summary(coef(mLassoOpt))[-1,1],
      summary(coef(mLassoOpt))[-1,3]) + xlab("gene") + ylab("beta-hat") + geom_hline(yintercept = 0, color = "red")


```

We have now our optimal Lasso model with AUC 0.82.

# Rigde regression

The last model that we will fit on the training data is the Ridge regression model that uses the $L_2$-norm. The major difference between Lasso and ridge regression is that ridge regression does not perform feature selection. We will again fit the model by using cross validation with 10 folds. The optimal $\lambda$ is much bigger compared to the Lasso regression model. The AUC value equals. 0.78.

```{r, echo=FALSE, cache = TRUE, out.width="50%"}
# Rigde regression
mRidge <- glmnet(x = as.matrix(trainX), y = as.matrix(trainY), alpha = 0, family = "binomial")
plot(mRidge, xvar = "lambda")

# Rigde regression with cross-validation
set.seed(44)
mCvRidge <- cv.glmnet(x = as.matrix(trainX), y = as.matrix(trainY), alpha = 0, family  = "binomial", type.measure = "auc", nfolds = K) # alp = 0 -> ridge
#plot(mCvRidge)

# Ridge regression with optimal lambda
rlambda_cv <- mCvRidge$lambda.min # optimal lambda
id_rlambda_cv <- which(mCvRidge$lambda == rlambda_cv)
auc_ridge <- mCvRidge$cvm[id_rlambda_cv]
mRidgeOpt <- glmnet(x = as.matrix(trainX), y = as.matrix(trainY), alpha = 0, family = "binomial", lambda = rlambda_cv)
#summary(coef(mRidgeOpt))
mCvRidge$lambda.min
auc_ridge

```

## Model evaluation

We decide to continue with the optimal Lasso regression model because we found an high AUC value and because Lasso regression is easier to interpret as it does feature selection.
We now want to evaluate this final model on our test set. We can first look at the corresponding ROC curve.

```{r help, echo=FALSE,  out.width="50%", warning=FALSE}
# plot roc curve for Lasso
#BiocManager::install("plotROC")
testX <- X_raw[-trainset, ]
testY <- Y[-trainset]
library(plotROC)
library(ggplot2)
dfLassoOpt <- data.frame(
  pred = predict(mCvLasso, s = lambda_cv, newx = as.matrix(testX), family = "binomial", type = "response") %>% c(.),
  known.truth = testY)

roc <-
  dfLassoOpt  %>%
  ggplot(aes(d = known.truth, m = pred)) +
  geom_roc(n.cuts = 0) +
  xlab("1-specificity (FPR)") +
  ylab("sensitivity (TPR)")
roc

calc_auc(roc)
```

The next step is to determine a good threshold $c$ as a prediction cut-off. This cutoff is chosen as to minimize the misclassification error. For this, we will use our test data. We are interested in the sensitivity ($\frac{TP}{TP+FN}$) and specifity ($\frac{TN}{TN+FP}$) of the final model. We find the following cutoff and corresponding sensitivity and specificity. 

```{r, echo=FALSE}
# lasso on test data
lasso_preds <- predict(mCvLasso, s = mCvLasso$lambda.min, newx = as.matrix(testX), family="binomial",type = "response")

# Inputs:
# * obs: vector of test observations
# * pred: vector of model predictions
# * cutoff_values: vector of prediction thresholds c
calculate_misclass_error <- function(obs, pred, cutoff_values) {
  stopifnot(length(obs) == length(pred))
  misclass_errors <- rep(NA, length(cutoff_values))
  for (i in seq_along(cutoff_values)) {
    cutoff <- cutoff_values[i]
    ypred <- as.numeric(pred > cutoff) # translates TRUE/FALSE to 1/0
    misclass_errors[i] <- mean(ypred != obs) # proportion of misclassifications
  }
  data.frame(
    "cutoff" = cutoff_values,
    "misclass" = misclass_errors
  )
}
result <- calculate_misclass_error(testY, lasso_preds, seq(0.4, 0.9, by = 0.01))
id <- which.min(result[, 2])
cutoff <- result[id, 1]
cutoff
newY <- ifelse(lasso_preds > cutoff, 1, 0)  # everything bigger than cutoff -> predict 1
#newY

# calculate sensitivity and specificity
sens <- sum(newY == 1 & testY==1)/ sum(testY == 1)
spec <- sum(newY == 0 & testY==0)/ sum(testY == 0)
sens 
spec
```


## Conclusions


## References
<div id="refs"></div>

## Appendix: Names of candidate differentiated genes
`r lfdr_genes`
