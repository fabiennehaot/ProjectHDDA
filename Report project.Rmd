---
title: "HDDA Project"
author: "Lisa Barbier, Klara Dewitte, Fabienne Haot, Kasimir Putseys"
date: "December 2021"
bibliography: references.bib
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(gridExtra)
library(HDDAData)
library(glmnet)
library(MASS)
library(locfdr)
library(boot)
library(pROC)
library(Rcpp)
library(plotROC)
library(ggplot2)
library(gridExtra)
library(grid)
```

# Executive Summary

We investigated the claim that some genes are responsible for a patientâ€™s likelihood of rejecting a kidney after transplantation. We started with an exploration of the data set by looking at the directions in the data space with the most variability, but this did not help us to make a distinction between the rejected and accepted cases. The variability in the data set does not appear to be related with kidney rejection. Next, we searched again for a linear combination of genes that succeeds in separating the two cases by using linear discriminant analysis. We noticed that we did not need all the genes. A subset of the total number of genes is enough to find a separation. This gives an indication that some genes are indeed associated with kidney rejection, while others are irrelevant. We continued our research by looking at which genes are differentially expressed between the two groups (by comparing means). We found 422 differentiated genes which are given in the appendix. From this result, we further assessed if kidney rejection can be predicted from the genes and this seemed to be the case. With only 12 genes, fairly-well predictions can be made (approximately 64% of the rejected kidneys could be predicted from these genes).

# Technical Report

## Data

Data description in [@einecke:2010]

```{r getdata, include = FALSE}
data("Einecke2010Kidney")
X_raw <- Einecke2010Kidney[, -1]
Y <- factor(Einecke2010Kidney[, 1], 
            levels = c(0,1), labels = c('accept', 'reject'))
```

## Exploration of the Data
The data exploration was started with examining the Einecke 2010 kidney data. Of the 250 observations, there are 67 cases where the kidney transplant was rejected and 183 where it was accepted. Furthermore, no missing values were detected. Next, we take a loook at possible outliers. Figure 1 illustrates that the data seems to be close to centered, but not scaled, which indicates centering and scaling will be needed for further analysis. Finally, no outliers were found. Figure 1 gives the minimum, mean and maximum expression level for every gene. We clearly see that three clouds are formed for the three values.

```{r soundness checks, echo = FALSE}
# missing values
nr_values  <- sapply(X_raw, function(var) {length(var)})
nr_missing <- sapply(X_raw, function(var) {length(which(is.na(var)))})
nr_unique  <- sapply(X_raw, function(var) {n_distinct(var)})

# outliers
max_per_var <- sapply(Einecke2010Kidney, max)
min_per_var <- sapply(Einecke2010Kidney, min)
mean_per_var <- sapply(Einecke2010Kidney, mean)

tb_summary <- tibble(Name = colnames(Einecke2010Kidney),
                     Nr = seq_along(Einecke2010Kidney),
                     MaxPerVar = max_per_var,
                     MinPerVar = min_per_var,
                     MeanPerVar = mean_per_var)
                     
ggplot(data = tb_summary) +
  geom_point(mapping = aes(x = Nr, y = MinPerVar), col = 'blue', size = .3) +
  geom_point(mapping = aes(x = Nr, y = MeanPerVar), col = 'green', size = .3) +
  geom_point(mapping = aes(x = Nr, y = MaxPerVar), col = 'red', size = .3) +
  ggtitle("Figure 1") +
  theme_bw()

X <- scale(X_raw)
```

A first look at the data does not provide a clear understanding on how kidney rejection is associated with the gene expression levels. Considering the data is in a high dimensional setting, we opted for PCA to obtain a better understanding.


```{r PCA, echo = FALSE, warning = FALSE, fig.height = 5, fig.width = 10, fig.align = "center"}
# PCA
svd_x <- svd(X)
Z <- svd_x$u %*% diag(svd_x$d) # Calculate the scores
V <- svd_x$v                 # Calculate the loadings
pca_x <- prcomp(X, center = FALSE, scale. = FALSE)

par(pch = 19, mfrow = c(1, 2))
var_explained <- svd_x$d^2 / sum(svd_x$d^2)
plot(var_explained,
     type = "b", ylab = "Percent variance explained", xlab = "PCs",
     col = 2
)

Scores <- X %*% svd_x$v
tb_scores <- tibble(PCA1 = Scores[, 1],
                    PCA2 = Scores[, 2],
                    Object = rownames(Scores),
                    Rejected = as.factor(Y))

ggplot(tb_scores) +
  geom_point(mapping = aes(x = PCA1, y = PCA2, col = Rejected)) +
  scale_color_manual(values = c("accept" = "red", "reject" = "blue")) +
  labs(title = "Figure 2: Scores for 2 PCA's") +
  theme_bw()   

p_dens <- ggplot(tb_scores) +
  geom_density(mapping=aes(x = PCA1 + PCA2, fill = Rejected),
               alpha = 0.5) +
  labs(title = "Figure 3: Density for 2 PC's: densities") +
  scale_fill_manual(values = c('red', 'blue')) +
  theme_bw()  

grid.arrange(p_scatter, p_dens, ncol = 2)
```

The first 2 PCs explain 19% of the variability of X and with the first 50 PCs we obtain 63% of the variability of PCs. 

To get more insight in the research question, we examine whether kidney rejection is associated with the PCs. For this, we look Figure 2. 
We notice that the PCs do not clearly separate the accepted and rejected cases. However, we do see that the rejected cases are more present in the bottom left corner and the accepted cases in the top right corner. + interpretation figure 3

Next, it is interesting to get more information on which genes are driving the PCs. A lot of the PCs are very small (near zero) and therefore, we create a histogram of the loadings to get a better visualization of these loadings.

```{r loadings, echo = FALSE, , fig.height = 5, fig.width = 10, fig.align = "center"}
par(pch = 19, mfrow = c(1, 2))
hist(V[, 1], breaks = 50, xlab = "PC 1 loadings", main = "Figure 4: Histogram PC 1 loadings")
abline(v = c(
  quantile(V[, 1], 0.05),
  quantile(V[, 1], 0.95)
), col = "red", lwd = 2)

hist(V[, 2], breaks = 50, xlab = "PC 2 loadings", main = "Figure 5: Histogram PC 2 loadings")
abline(v = c(
  quantile(V[, 2], 0.05),
  quantile(V[, 2], 0.95)
), col = "red", lwd = 2)
```

The histograms (Figures 4 and 5) confirm this finding, which gives us a reason to believe that sparse PCA is worthwhile. 

```{r sparce PCA, echo = FALSE, fig.height = 5, fig.width = 10, fig.align = "center"}
par(pch = 19, mfrow = c(1, 2))
set.seed(45)
fit_loadings1 <- cv.glmnet(X, Z[, 1],
                           alpha = 0.5, nfolds = 5)
set.seed(45)
fit_loadings2 <- cv.glmnet(X, Z[, 2], alpha = 0.5, nfolds = 5)

sparse_loadings1 <- as.vector(coef(fit_loadings1, s = fit_loadings1$lambda.1se))
sparse_loadings2 <- as.vector(coef(fit_loadings2, s = fit_loadings2$lambda.1se))
## How many non-zero loadings do we have (excluding the intercept)?
non_zero1 <- sum(abs(sparse_loadings1[-1]) > 0)
non_zero2 <- sum(abs(sparse_loadings2[-1]) > 0)

SPC1 <- X %*% sparse_loadings1[-1] # without the intercept
SPC2 <- X %*% sparse_loadings2[-1] # without the intercept

par(mfrow = c(1, 2))
cols <- c("0" = "red", "1" = "blue")
plot(Z[, 1], Z[, 2],
     col = cols[Y], xlab = "PC1", ylab = "PC2", pch = 16,
     main = "Figure 6:All genes \nfor PC1 and PC2"
)

plot(SPC1, SPC2,
     col = cols[Y], xlab = "SPC1", ylab = "SPC2", pch = 16,
     main = paste("Figure 7:", non_zero1, "genes for SPC1 \n and", non_zero2, "genes for SPC2")
)
```

For both PC1 and PC2 around 200 genes are important (Figure 7), which can provide more or less the same result as the PCA with all the genes (Figure 6).

Finally, because PCA only takes the genes into account in the decomposition and not the class membership, we executed LDA. Furthermore, because the genes are collinear, a sparse LDA is performed as well. The results of these are illustrated in Figures 8 and 9.


```{r LDA, warning = FALSE, echo = FALSE, fig.height = 5, fig.width = 10, fig.align = "center"}
kidney.lda <- MASS::lda(x = X, grouping = Y)
Vlda <- kidney.lda$scaling
colnames(Vlda) <- paste0("V", 1:ncol(Vlda))
Zlda <- X%*%Vlda
colnames(Zlda) <- paste0("Z", 1:ncol(Zlda))
par(mfrow = c(1, 2))
boxplot(Zlda ~ Y, col = cols, ylab = expression("Z"[1]),
        main = "Figure 8: LDA with total set of genes")
set.seed(45)
lda_loadings <- cv.glmnet(X, Zlda, alpha = 0.5, nfolds = 5)

sparse_lda_loadings <- as.vector(
  coef(lda_loadings, s = lda_loadings$lambda.1se)
)

SLDA <- X %*% sparse_lda_loadings[-1]

# number of non-zero loadings
n_nonzero <- sum(sparse_lda_loadings != 0)
boxplot(SLDA ~ Y,
        col = cols, ylab = "SLDA",
        main = sprintf("Figure 9: LDA with subset of %d genes", n_nonzero)
)
```

Both the regular LDA (Figure 8) and the sparse LDA (Figure 9) separate the accepted cases (red) and the rejected cases (blue) fairly well. However, the sparse LDA only needs 52 genes to do this. With the sparse LDA we see that the accepted cases are situated just below 0 and the rejected cases around 1.

## Hypothesis Testing 
Some genes in the data set are an indicator for kidney rejection. In this section we look for genes which behave differently between the two groups. Since we do not know whether differentiated genes will show higher or lower intensities we opt for 2 sided t-tests with unequal variances. 
\[H_0: \mu_{normal}(gene) = \mu_{reject}(gene) \]
\[H_1: \mu_{normal}(gene) \neq \mu_{reject}(gene) \]

Because we are performing multiple testing we will adjust our p-values, the FDR will be controlled on 10%.

```{r hyptest_tttests, echo = FALSE}
# graphical parameters
fill_blue = "lightblue"
fill_red = "firebrick"
col_blue = "blue"
col_red = "red"

gene_data <- as.matrix(X_raw)
group <- Y
# non adjusted p-values 
ttest_results <- apply(gene_data, 2, function(x){
  t_test <- t.test(x ~ group)
  tibble(stat = t_test$statistic, 
         p_val = t_test$p.value, 
         df = t_test$parameter)})

t_stats <- bind_rows(ttest_results) %>%
  mutate(gene = colnames(X_raw),
         z_val= case_when(stat < 0 ~ qnorm(p_val/2), 
                            TRUE~ qnorm(1-p_val/2)))
```

```{r hyptest_unadjusted, echo = FALSE, fig.height = 5, fig.width = 10, fig.align = "center"}
# selection
nonadj_level <- 0.05
selected_nonadj <- t_stats %>% filter(p_val< nonadj_level)

# plot histogram for the unadjusted z-values
mean_z <- round(mean(t_stats$z_val), 3) 
sd_z <- round(sd(t_stats$z_val), 3) 
threshold_nonadj <- selected_nonadj %>% 
  summarize(left = max(case_when(z_val > mean_z ~ -Inf, TRUE~ z_val)),
            right = min(case_when(z_val < mean_z ~ +Inf, TRUE~ z_val)))

hist_zval <- ggplot(data = t_stats) + 
  geom_histogram(
    mapping = aes(x = z_val, after_stat(density)),
    bins = 20, alpha = .7, fill = fill_blue) +
  geom_vline(mapping = aes(xintercept = threshold_nonadj$left),
             color = col_red) +    
  geom_vline(mapping = aes(xintercept = threshold_nonadj$right), color = col_red)+
  geom_line(mapping = aes(x = z_val,y = dnorm(z_val)), color = col_blue) +
  labs(title = "A Histogram of  z-values",
       x = "z-values",
       y = "density") +
  annotate(geom = "text", x = -7, y = 0.38, 
           label = paste0("mean: ", mean_z, "\nstdev:", sd_z)) +
  theme_bw()

# signal histogram - current behaviour of p-values: uniform?
hist_pval <- ggplot(data = t_stats) +
  geom_histogram(mapping = aes(x = p_val),
                 fill = fill_red, bins = 20, alpha = .7) +
  labs(title = 'B Histogram of p-values', x = "p-value") +
  theme_bw()

grid.arrange(hist_zval, hist_pval, ncol = 2,
             top = "Figure 10: Multiple t-tests unadjusted")

# number of differentiated genes
table_p_nonadj <- table(t_stats$p_val < nonadj_level)
nr_detected_nonadj <- as.numeric(table_p_nonadj['TRUE'])
```

The standard normal curve is too narrow to fit the distribution (Figure 10A). This can happen when the genes are highly correlated or when the null hypothesis does not hold. The histogram of the p-values (Figure 10B) shows a clear peak for p-values lower than 5%, but there are too many p-values between 5% and 35%. We cannot execute the @BH:1995 procedue now, because this uses the standard normal null distribution to calculate the FDR.

```{r hyptest_fdr_padj, echo = FALSE, fig.height = 5, fig.width = 10, fig.align = "center"}
# adjusted p values
t_stats <- t_stats %>%
  mutate(z_scale = (z_val - mean(z_val))/sd(z_val),
         p_scale = pnorm(z_scale),
         p_adj = p.adjust(p_scale, method = "fdr"))

# these are the detected genes and a histogram of their t-values
fdr_level <- 0.1
selected_fdr <- t_stats %>% filter(p_adj < fdr_level)

# plot histogram
threshold_fdr <- selected_fdr %>% 
  summarize(left = max(case_when(z_scale > 0~-Inf, TRUE~ z_scale)),
            right = min(case_when(z_scale < 0~+Inf, TRUE~ z_scale)))

hist_fdr <- ggplot(t_stats) + 
  geom_histogram(mapping = aes(x = z_scale, after_stat(density)),
                             fill = fill_blue, bins = 20, alpha = 0.7) +
  geom_vline(mapping = aes(xintercept = threshold_fdr$left),
             color = col_red) +    
  geom_line(mapping = aes(x = z_scale, y = dnorm(z_scale)),
            color = col_blue) +
  labs(title = 'A Histogram of z-values') +
  geom_histogram(mapping = aes(x = z_scale, after_stat(density)),
                             fill = fill_blue, bins = 20, alpha = .7) +
  geom_vline(mapping = aes(xintercept = threshold_fdr$left), color = col_red) +    
  geom_line(mapping = aes(x = z_scale, y = dnorm(z_scale)), color = col_red) +
  theme_bw()            

# signal histogram - current behaviour of p-values: uniform?
hist_pscale <- ggplot(data = t_stats) +
  geom_histogram(mapping = aes(x = p_scale),
                 fill = fill_red, bins = 20, alpha = 0.7) +
  labs(title = 'B Histogram of p-values') +
  theme_bw()

# monotonous transformation:
pp_adj <- t_stats %>%
  ggplot(aes(x = p_scale,y = p_adj)) +
  geom_point(size = .3, color = col_blue) +
  geom_segment(x = 0, y = 0, xend = 1, yend = 1) +
  geom_hline(yintercept = fdr_level, color = col_red) +
  labs(title = "C Adjusting the p-values",
       y = "adjusted p-value") +
  theme_bw()

# show histogram z-values +  monotonous adjustment for the p-values
grid.arrange(hist_fdr, hist_pscale, ncol=2,
             top  = "Figure 11: Multiple t-tests: scaling and FDR-adjustment")

# number of detected genes
table_p_adj <- table(t_stats$p_adj < fdr_level)
nr_detected_fdr <- as.numeric(table_p_adj['TRUE'])
```

Standardizing the z-values does not help, the theoretical distribution is now too wide to apply the @BH:1995 procedure.  
We  will use the local approach instead, where we assume that the observed histogram represents a mixture of different Gaussian distributions (@Efron:2007). The parameters  of the Gaussian distributions will be estimated with maximum likelihood.  

```{r localfdr, echo = FALSE, fig.height = 5, fig.width = 10, fig.align = "center"}
#local fdr
fdr_x <- locfdr(t_stats$z_val, plot = 4, main = "A: histogram")  #add here maintitle and change title other plots?
prop_h0 <- fdr_x$fp0['mlest', 'p0']

#control FDR for 10%
FDR_left <- fdr_x$mat[, 'Fdrleft']
z_mat <- fdr_x$mat[, 'x']
lfdr_mat <- fdr_x$mat[, 'fdr']
dens1_mat <- fdr_x$mat[, 'p1f1'] 

id <- which.max(FDR_left[FDR_left < fdr_level])
t_int <- (fdr_level - FDR_left[id])/(FDR_left[id+1] - FDR_left[id])
threshold <- z_mat[id] * (1-t_int) + z_mat[id] * t_int 
lfdr_level <- lfdr_mat[id] * (1-t_int) + lfdr_mat[id] * t_int
cdf_true_positive = sum(dens1_mat[1:id]) + dens1_mat[id+1] * t_int
prop_tp_r <- cdf_true_positive/sum(dens1_mat)
```

The left plot shows the density of both populations. The proportion of differentiated genes is expected to be `r round(100*(1 - prop_h0),2)`% (`r round(10000*(1 - prop_h0),0)` genes) for our data set.  The second plot shows the local false discovery rate. Since we want to control the FDR we are interested in the red dashed line. The threshold is the value on the x-axis corresponding with height 10%. This threshold is `r round(threshold, 2)`.  From the third graph we can deduce that for an FDR of 10% or a local fdr-level of `r round(100* lfdr_level,1)`%, we expect `r round(100*prop_tp_r)`% of all differentiated genes to be correctly identified.   

```{r lfdr_genes, echo = FALSE, fig.height = 5, fig.align = "center"}
t_stats <- t_stats %>%
  mutate(
    lfdr = fdr_x$fdr,
    zfdr = (lfdr < lfdr_level) * z_val,
    isdetected_lfdr = factor(lfdr < lfdr_level,
                        levels= c(TRUE, FALSE),
                        labels = c('detected', 'H0 not rejected')))
summ_lfdr <- t_stats %>%
  filter(lfdr < lfdr_level) %>%
  summarize(nr_of_genes = n(), mean_lfdr = mean(lfdr))

data_lfdr <- as_tibble(fdr_x$mat)
plot_lfdr <- ggplot(data_lfdr) + 
  geom_col(mapping = aes(x = x, y = counts/sum(counts)),
           fill = fill_blue) +
  geom_vline(xintercept = threshold, col = col_red) +
  geom_line(mapping = aes(x = x, y = f0/sum(f0)),
            col = col_blue) +
  labs(x = 'z-value', y = 'density') +
  theme_bw()       

grid.arrange(plot_lfdr, top = "Figure 12: Multiple t-tests: Local FDR")

lfdr_genes <- t_stats %>% filter(lfdr < lfdr_level)%>% .$gene
```

We have found `r summ_lfdr$nr_of_genes` candidates for differentiated genes. We expect to have about `r round(fdr_level* summ_lfdr$nr_of_genes)` falsely detected genes. The genes detected by the local fdr will be added as an appendix.  

## Model Selection

In the next step we want the predict the rejection status using the gene expression levels. Therefore, we first start with splitting the dataset into a train (70%) and test (30%) dataset. The training set will be used to train the model and tune the hyperparameters (for example the penalty parameter in lasso regression), while the test set will be used to evaluate the out-of-sample performance of our final model. We will center and scale the training set. Afterwards, the same scale and translation factor are applied on the test set.

```{r training_and_test_data, include = FALSE}
set.seed(1234)
trainset <- sample(nrow(X_raw), 0.7*nrow(X_raw))
trainX <- X_raw[trainset, ]
trainX <- scale(trainX, center = TRUE, scale = TRUE)
scale_factor <- attr(trainX, "scaled:scale")
scale_translation <- attr(trainX, "scaled:center")
trainX <- as.data.frame(trainX)

dim(trainX) #175*10000
trainY <- Y[trainset]
testX <- X_raw[-trainset, ]
dim(testX) #75*10000
testY <- Y[-trainset]

for (varnr in seq_along(testX)){
    xbar <- scale_translation[varnr]
    xsd <- scale_factor[varnr]
    testX[[varnr]] <- (testX[[varnr]]-xbar )/xsd}
    
list(tr_x = trainX, tr_y = trainY, test_x = testX, test_y = testY)

train_data <- data.frame(trainY, trainX)
test_data <- data.frame(testY, testX)
```
Here, we will evaluate three prediction models: principal component regression (PCR), ridge regression and lasso regression.

### Principal compentent regression (PCR)

```{r PCR, include = FALSE}
# Calculate PCA and extract scores
pca_X <- prcomp(trainX)
Z <- pca_X$x

## Total number of available PCs
n_PC <- ncol(Z)
n_PC
```
First, the principal components are calculated and the total number of available PCs is selected. In our case, `r n_PC` PCs are available in the train dataset.

```{r fullPCR, include = FALSE, warning = FALSE}
fit_data <- data.frame(trainY, Z)

## Logistic regression with all the PC's
full_model <- suppressMessages(glm(trainY ~ ., data = fit_data, family = "binomial"))

set.seed(42)
full_model_cv <- suppressMessages(cv.glm(
  data = fit_data,  glmfit = full_model,
  cost = auc, K = 9  
))

full_model_cv$delta[1] 
```
First we try to fit a model with all the PCs, using crossvalidation. Via crossvalidation will the training set be divived into 9 approximately equal subsets. Here we get an area under the curve (AUC) of `r full_model_cv$delta[1]`. However, because we use all PCs, we got a warning for overfitting the data.

```{r eachPC, echo = FALSE, warning = FALSE, fig.height = 5, fig.width = 10, fig.align = "center"}
auc_cv_glm <- function(trials, data, folds){
  full_model <- suppressWarnings(glm(Y ~ ., data = data, family = "binomial"))
  n_trials <- length(trials)
  error_auc <- rep(0, n_trials)
  comp <- 1
  for (k_try in trials){
    full_model_cv <- suppressWarnings(suppressMessages(
      cv.glm(
        data = data[1:(k_try+1)],  
        glmfit = full_model,
        cost = pROC::auc, 
        K = folds # note: specify the auc function (from pROC) without`()`!
      )))
    error_auc[comp] = full_model_cv$delta[1] 
    comp <- comp + 1
  }
  df_auc <- tibble(auc = error_auc, nr_of_pc = trials)
}

# model PCR for the first k principal components
pca_x <- prcomp(trainX)
Z <- pca_x$x
fit_data <- data.frame(Y = trainY, Z)

# 9-fold Cross-validation on this one particular model, using AUC
# 2 step-procedure to detect k: big steps (10) first
set.seed(42)
trials <- seq(1, nrow(fit_data), 10)
folds <- 9
cvpcr_run1 <- auc_cv_glm(trials, fit_data, folds)
nr_pc_opt1 <- cvpcr_run1 %>% filter(auc == max(auc)) %>% .$nr_of_pc # largest AUC for this number of PC's
ggplot(cvpcr_run1) +
  geom_line(mapping = aes(x = nr_of_pc, y = auc), col = 'blue') +
  geom_vline(mapping = aes(xintercept = nr_pc_opt1), col = 'firebrick')+
  labs(title = 'A cross validation PCR: steps of 10')+
  theme_bw() -> p1

# zoom in: small steps(1) - same seed
set.seed(42)
trials <- seq(1, 20)
folds <- 9
cvpcr_run2 <- auc_cv_glm(trials, fit_data, folds)
nr_pc_opt2 <- cvpcr_run2 %>% filter(auc == max(auc)) %>% .$nr_of_pc  #largest AUC for this number of PCs
ggplot(cvpcr_run2) +
  geom_line(mapping = aes(x = nr_of_pc, y = auc), col = 'blue')+
  geom_vline(mapping = aes(xintercept = nr_pc_opt2), col = 'firebrick')+
  labs(title = 'B cross validation PCR: steps of 1') +
  theme_bw() -> p2

grid.arrange(p1, p2, ncol = 2, top = "Figure 14") # plot the 2 plots next to each other
```

In a second step, we loop and repeat this for each PC. First, we loop over the 175 PCs, in steps of 10, to decide in which region we will zoom in. Here, we get `r nr_pc_opt1` as optimal number of PCs. To get a more accurate look, we zoom in between 1 and 20 PCs and repeat the same process, but wit steps of 1. This gives us an optimal number `r nr_pc_opt2` PCS.

```{r PCRfinal, echo = FALSE, warning = FALSE, fig.align = "center"}
pca <- prcomp(trainX)
Vk <- pca$rotation[, 1:nr_pc_opt2] # the loadings matrix
Zk <- pca$x[, 1:nr_pc_opt2]
pcr_model1 <- glm(trainY ~ Zk, family = "binomial")
summary(pcr_model1)

pcr_train = tibble(pcr_score = pcr_model1$fitted.values, 
                   rejection = trainY)
                   
p_dens_pcr <- ggplot(pcr_train) +
  geom_density(mapping = aes(x = pcr_score, fill=rejection), 
               alpha = 0.5) +
  labs(title = "Figure 15: Density for 2 PC's: densities") +
  scale_fill_manual(values = c('firebrick', 'lightblue')) +
  theme_bw() 

p_dens_pcr
```

In the last step, a principle component regression model with `r nr_pc_opt2` PCs is fitted. The Akaike's Information Criterion (AIC) for this model is 158.43.
When we plot the two densities of the accepted and rejected kidneys with this model, we clearly see there is no overlap. This means, the model is not acceptable enough. 

### Lasso regression

We are now going to use two penalised (logistic) regression models. We start with Lasso regression that uses the $L_1$-norm. We will again only use the training set to fit the model. The following figure nicely shows that when the penalty parameter $\lambda$ increases the estimates are shrunken towards zero. When it hits zero, it remains zero. Hence, choosing $\lambda$ is related to feature selection.


```{r Lasso, echo = FALSE, fig.show = "hold", out.width = "50%"}
# Lasso
mLasso <- glmnet(x = trainX, y = trainY, alpha = 1, family  = "binomial")  # alpha = 1 -> Lasso
plot(mLasso, xvar = "lambda")

# cross validation to find optimal lambda
set.seed(44)
K <- 10 # number of folds
mCvLasso <- cv.glmnet(x = as.matrix(trainX), y = as.matrix(trainY), alpha = 1, family  = "binomial", type.measure = "auc", nfolds = K)
plot(mCvLasso)
```

To make a decision about $\lambda$, we will use cross-validation on the training data set. We use 10-fold cross-validation because our data set is quite small, so it should not take too much time. We optimize the area under the receiver characteristic curve (AUC).

Two vertical lines are added on the figure. The first one gives the $\log(\lambda)$ for which we find the highest AUC. We will go on with this value, because with the other choice for $\lambda$, almost no genes are used anymore. We are left with 12 predictors from the original 10,000 genes (sparse solution). The values of the corresponding $\beta$ coefficients are shown on the following plot and next to the figure, we show the corresponding names of the genes. These genes are hence the most important in predicting the kidney transplant rejection.

```{r Lasso optimal, echo = FALSE, out.width = "50%"}
# Lasso with optimal lambda
lambda_cv <- mCvLasso$lambda.min # optimal lambda
id_lambda_cv <- which(mCvLasso$lambda == lambda_cv)
auc_lasso <- mCvLasso$cvm[id_lambda_cv]

# Optimal model 
mLassoOpt <- glmnet(x = as.matrix(trainX), y = as.matrix(trainY), alpha = 1, family  = "binomial", lambda = lambda_cv)
#summary(coef(mLassoOpt))

# with optimal lambda (largest auc) the output shows the non-zero estimated regression coefficients
library(ggplot2)
qplot(summary(coef(mLassoOpt))[-1,1],
      summary(coef(mLassoOpt))[-1,3]) + xlab("gene") + ylab("beta-hat") + geom_hline(yintercept = 0, color = "red")

#colnames(X_raw)[summary(coef(mLassoOpt))[[1]]]
grid.newpage()
grid.table(colnames(X_raw)[summary(coef(mLassoOpt))[[1]]], theme = ttheme_minimal())

```

We have now our optimal Lasso model with $\lambda =$ `r round(lambda_cv,3)` and AUC `r round(auc_lasso,2)` on the training data set, which is quite high.

### Rigde regression

```{r ridge, echo = FALSE, out.width = "50%"}
# Rigde regression
mRidge <- glmnet(x = as.matrix(trainX), y = as.matrix(trainY), alpha = 0, family  = "binomial")
plot(mRidge, xvar = "lambda")

# Rigde regression with cross-validation
set.seed(44)
mCvRidge <- cv.glmnet(x = as.matrix(trainX), y = as.matrix(trainY), alpha = 0, family  = "binomial", type.measure = "auc", nfolds = K) # alpha = 0 -> ridge
#plot(mCvRidge)

# Ridge regression with optimal lambda
rlambda_cv <- mCvRidge$lambda.min # optimal lambda
id_rlambda_cv <- which(mCvRidge$lambda == rlambda_cv)
auc_ridge <- mCvRidge$cvm[id_rlambda_cv]
mRidgeOpt <- glmnet(x = as.matrix(trainX), y = as.matrix(trainY), alpha = 0, family  = "binomial", lambda = rlambda_cv)
#summary(coef(mRidgeOpt))

```

The last model that we will fit on the training data is the ridge regression model that uses the $L_2$-norm. The major difference between lasso and ridge regression is that ridge regression does not perform feature selection. We will again fit the model by using cross validation with 10 folds. The optimal $\lambda$ is much higher compared to the Lasso regression model (`r round(rlambda_cv,3)`). The AUC value on the training data set equals `r round(auc_ridge, 2)` which is lower than for the lasso regression model.

## Model evaluation

We decide to continue our analysis with the optimal Lasso regression model because it has the highest AUC value of the 3 implemented models and because it is easier to interpret as it does feature selection.
Next, we evaluate this final model on our test set. First, the corresponding ROC curve is examined.

```{r rocLasso, echo = FALSE,  out.width = "50%", warning = FALSE}
# plot roc curve for Lasso
dfLassoOpt <- data.frame(
  pred = predict(mCvLasso, s = lambda_cv, newx = as.matrix(testX), family = "binomial", type = "response") %>% c(.),
  known.truth = testY)

roc <-
  dfLassoOpt  %>%
  ggplot(aes(d = known.truth, m = pred)) +
  geom_roc(n.cuts = 0) +
  xlab("1-specificity (FPR)") +
  ylab("sensitivity (TPR)")
roc

#calc_auc(roc)
```

```{r misclassification op training data, echo = FALSE}
# predictions on training set
lasso_preds_train <- predict(mCvLasso, s = mCvLasso$lambda.min, newx = as.matrix(trainX), family = "binomial",type = "response")

# Inputs:
# * obs: vector of test observations
# * pred: vector of model predictions
# * cutoff_values: vector of prediction thresholds c
calculate_misclass_error <- function(obs, pred, cutoff_values) {
  stopifnot(length(obs) == length(pred))
  misclass_errors <- rep(NA, length(cutoff_values))
  for (i in seq_along(cutoff_values)) {
    cutoff <- cutoff_values[i]
    ypred <- as.numeric(pred > cutoff) # translates TRUE/FALSE to 1/0
    misclass_errors[i] <- mean(ypred != obs) # proportion of misclassifications
  }
  data.frame(
    "cutoff" = cutoff_values,
    "misclass" = misclass_errors
  )
}
# make sure we have the right levels in the training en test set
levels(trainY)[levels(trainY) == "accept"] <- 0
levels(trainY)[levels(trainY) == "reject"] <- 1
levels(testY)[levels(testY) == "accept"] <- 0
levels(testY)[levels(testY) == "reject"] <- 1

# optimize cutoff on the training set
result <- calculate_misclass_error(trainY, lasso_preds_train, seq(0.1, 0.9, by = 0.01))
id <- which.min(result[, 2])
cutoff <- result[id, 1]  # cutoff optimized on training data
cutoff

# how does it perform on the test set with the chosen cutoff
lasso_preds_test <- predict(mCvLasso, s = mCvLasso$lambda.min, newx = as.matrix(testX), family="binomial",type = "response")
newY <- ifelse(lasso_preds_test > cutoff, 1, 0)  # everything bigger than cutoff -> predict 1

# calculate sensitivity and specificity
sens <- sum(newY == 1 & testY==1)/ sum(testY == 1)
spec <- sum(newY == 0 & testY==0)/ sum(testY == 0)
```

A high AUC value is also found on the test set (`r round(calc_auc(roc)[1,3],2)`). The next step is to determine a good threshold $c$ as a prediction cut-off. This cutoff is chosen in order to minimize the misclassification error. For this, we will use our training data again. Once the cutoff is decided, we want to assess the performance of our final model on a test set that contains observations we did not use to fit the model. The goal is is to understand how well it predicts the right group for new observations. For this, we are interested in the sensitivity ($\frac{TP}{TP+FN}$) and specifity ($\frac{TN}{TN+FP}$) of the final model. With the optimized cutoff `r cutoff`, we obtain sensitivity `r round(sens,2)` and specificity `r round(spec,2)`. Both measures are satisfactory. The specificity is higher than the sensitivity which means the model does a better job in predicting the accepted cases than the rejected ones.


## References
<div id="refs"></div>

## Appendix: Names of candidate differentiated genes
`r lfdr_genes`
